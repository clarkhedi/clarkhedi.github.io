<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.5"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.5"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><link rel="stylesheet" type="text/css" href="https://unpkg.com/gitalk/dist/gitalk.css?v=2.0.5"><title>TensorFlow 基本概念及构建 | 我中意你23332</title><meta name="generator" content="Hexo 5.2.0"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">TensorFlow 基本概念及构建</h1><a id="logo" href="/.">我中意你23332</a><p class="description">Tomorrow comes never.</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">TensorFlow 基本概念及构建</h1><div class="post-meta"><a href="/2020/04/09/tensorflow-ji-ben-gai-nian-ji-gou-jian/#comments" class="comment-count"></a><p><span class="date">Apr 09, 2020</span><span><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="category">深度学习笔记</a><a href="/categories/%E5%AE%9E%E9%AA%8C%E6%A5%BC%E5%AD%A6%E4%B9%A0/" class="category">实验楼学习</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><h4 id="总体介绍"><a href="#总体介绍" class="headerlink" title="总体介绍"></a>总体介绍</h4><p>目前深度学习异常的火热，而深度学习模型的搭建需要依赖于深度学习框架，TensorFlow 就是其中的一种非常流行的深度学习框架。因此，想要学习深度学习算法，学习 TensorFlow 十分必要。而本次主要介绍 TensorFlow 的基本概念以及基本使用方法。</p>
<h4 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h4><ul>
<li>TensorFlow</li>
<li>张量 Tensor</li>
<li>计算图 Graph</li>
<li>线性回归实现</li>
<li>模型保存 Save</li>
</ul>
<h3 id="TensorFlow-介绍"><a href="#TensorFlow-介绍" class="headerlink" title="TensorFlow 介绍"></a>TensorFlow 介绍</h3><p>TensorFlow 是目前最强大的深度学习框架之一，由 Google 团队主导开发，并在 2015 年进行开源。因此，TensorFlow 拥有非常活跃的社区。这意味着当你在使用 TensorFlow 遇到问题时，往往在许多搜索引擎中搜索相关的报错信息就能找到答案。</p>
<p><img src="https://dn-simplecloud.shiyanlou.com/questions/uid958100-20190617-1560756115227" alt="img"></p>
<p>经过几年的不断优化和发展，TensorFlow 目前的代码量大约在 40 万行左右。因此，本系列不可能涵盖 TensorFlow 所有的内容，仅介绍其常用的操作或 API，具体如下：</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/guide/low_level_intro"> <em>简介</em></a> - 介绍了如何使用高阶 API 之外的低阶 TensorFlow API 的基础知识。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/guide/tensors"> <em>张量</em></a> - 介绍了如何创建、操作和访问张量（TensorFlow 中的基本对象）。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/guide/variables"> <em>变量</em></a> - 详细介绍了如何在程序中表示共享持久状态。</p>
</li>
<li><p><em>图和会话</em></p>
</li>
</ul>
<p>  - 介绍了以下内容：</p>
<ul>
<li>数据流图：这是 TensorFlow 将计算表示为操作之间的依赖关系的一种表示法。</li>
<li>会话：TensorFlow 跨一个或多个本地或远程设备运行数据流图的机制。如果您使用低阶 TensorFlow API 编程，请务必阅读并理解本单元的内容。如果你使用高阶 TensorFlow API（例如 Estimator 或 Keras）编程，则高阶 API 会为你创建和管理图和会话，但是理解图和会话依然对你有所帮助。</li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/guide/saved_model"> <em>保存和恢复</em></a> - 介绍了如何保存和恢复变量及模型。</li>
</ul>
<p>虽然使用 TensorFlow 的高阶 API 来搭建模型会更简单。但是由于其是低阶 API 的高层封装，所以往往更难调试。所以本次实验主要讲解低阶 API。</p>
<h3 id="数据流图"><a href="#数据流图" class="headerlink" title="数据流图"></a>数据流图</h3><p>与其他科学计算库不一样，在 TensorFlow 中，每个运算操作都可以看做是一个计算图，如下图所示。</p>
<p><img src="https://tensorflow.google.cn/images/tensors_flowing.gif" alt="img"></p>
<p>TensorFlow 使用数据流图将计算表示为独立的指令之间的依赖关系。这可生成低级别的编程模型，在该模型中，首先定义数据流图，然后创建 TensorFlow 会话，以便在一组本地和远程设备上运行所构建计算图的各个部分。</p>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Dataflow_programming"> <em>数据流</em></a> 是一种用于并行计算的常用编程模型。在数据流图中，节点表示计算单元，边表示计算使用或产生的数据。例如，在 TensorFlow 图中，<a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/matmul"><code>tf.matmul</code></a> 操作对应于单个节点，该节点具有两个传入边（要相乘的矩阵）和一个传出边（乘法结果）。</p>
<p>在执行程序时，数据流可以为 TensorFlow 提供多项优势：</p>
<ul>
<li>并行处理。 通过使用明确的边来表示操作之间的依赖关系，系统可以轻松识别能够并行执行的操作。</li>
<li>分布式执行。 通过使用明确的边来表示操作之间流动的值，TensorFlow 可以将程序划分到连接至不同机器的多台设备上（CPU、GPU 和 TPU）。TensorFlow 将在这些设备之间进行必要的通信和协调。</li>
<li>编译。 TensorFlow 的 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/performance/xla/index"> <em>XLA 编译器</em></a> 可以使用数据流图中的信息生成更快的代码，例如将相邻的操作融合到一起。</li>
<li>可移植性。 数据流图是一种不依赖于语言的模型代码表示法。你可以使用 Python 构建数据流图，将其存储在 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/guide/saved_model"> <em>SavedModel</em></a> 中，并使用 C++ 程序进行恢复，从而实现低延迟的推理。</li>
</ul>
<p>在 TensorFlow 中，数据流图是一个 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Graph"><code>tf.Graph</code></a> 对象，<a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Graph"><code>tf.Graph</code></a> 包含两类相关信息：</p>
<ul>
<li>图结构： 图的节点和边，表示各个操作组合在一起的方式，但不规定它们的使用方式。图结构与汇编代码类似：检查图结构可以传达一些有用的信息，但它不包含源代码传达的所有实用上下文信息。</li>
<li>图集合： TensorFlow 提供了一种在 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Graph"><code>tf.Graph</code></a> 中存储元数据集合的通用机制。<a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/add_to_collection"><code>tf.add_to_collection</code></a> 函数允许将对象列表与一个键关联（其中 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/GraphKeys"><code>tf.GraphKeys</code></a> 定义了部分标准键），<a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/get_collection"><code>tf.get_collection</code></a> 允许查询与某个键关联的所有对象。TensorFlow 库的许多部分会使用此设施资源：例如，当创建 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Variable"><code>tf.Variable</code></a> 时，系统会默认将其添加到表示 “全局变量” 和 “可训练变量” 的集合中。当后续创建 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/train/Saver"><code>tf.train.Saver</code></a> 或 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/train/Optimizer"><code>tf.train.Optimizer</code></a> 时，这些集合中的变量将用作默认参数。</li>
</ul>
<h4 id="构建-tf-Graph"><a href="#构建-tf-Graph" class="headerlink" title="构建 tf.Graph"></a>构建 <code>tf.Graph</code></h4><p>大多数 TensorFlow 程序都以数据流图构建阶段开始。在 TensorFlow 中，我们可以使用 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Graph"><code>tf.Graph</code></a> 来创建一个图。例如下面代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">g_1 = tf.Graph()    <span class="comment"># 定义一个图</span></span><br><span class="line"><span class="keyword">with</span> g_1.as_default():  <span class="comment"># 在 g_1 图下创建节点</span></span><br><span class="line">    a = tf.constant(<span class="number">3.0</span>, name=<span class="string">&#x27;a&#x27;</span>)  <span class="comment"># 创建一个常量 a</span></span><br><span class="line">    b = tf.constant(<span class="number">4.0</span>, name=<span class="string">&#x27;b&#x27;</span>)  <span class="comment"># 创建一个常量 b</span></span><br><span class="line">    c = a + b  <span class="comment"># 创建一个操作 a + b</span></span><br></pre></td></tr></table></figure>

<p>在上面的代码中，我们创建了一个图 <code>g_1</code> ，并在该图中添加两个节点 <code>a</code> 和 <code>b</code>。然后对两者进行相加得到 <code>c</code>。如果使用 TensorBoard 可以将上面所构建的图打印出来，如下图所示.因 TensorBoard 在没有展示代码，如果你感兴趣可以在现在运行 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/guide/graph_viz"> <em>官方文档</em></a> 提供的案例。</p>
<p><img src="https://dn-simplecloud.shiyanlou.com/questions/uid958100-20190617-1560753446827" alt="&lt;i class=&quot;fa fa-external-link-square&quot; aria-hidden=&quot;true&quot;&gt; 图片描述&lt;/i&gt;"></p>
<p>现在我们打印出这三个节点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>

<p>从上面的输出结果可以看到，我们的输出结果为三个形如 <code>Tensor(&quot;a:0&quot;, shape=(), dtype=float32)</code> 的 Tensor 对象，其中 <code>&quot;a:0&quot;</code> 表示节点名称，<code>shape=()</code> 表示节点的形状，<code>dtype=float32</code> 为节点的数据类型。</p>
<p>一般情况下 TensorFlow 提供了一个默认图，而且大多数程序仅依赖于默认图。所以如果你的代码中只创建一个运算图，则不需要自己手动创建。</p>
<h4 id="命名空间"><a href="#命名空间" class="headerlink" title="命名空间"></a>命名空间</h4><p><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Graph"><code>tf.Graph</code></a> 对象会定义一个命名空间（为其包含的 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Operation"><code>tf.Operation</code></a> 对象）。TensorFlow 会自动为数据流图中的每个指令选择一个唯一名称，也可以指定描述性名称，使程序阅读和调试起来更轻松。TensorFlow API 提供两种方法来覆盖操作名称：</p>
<ul>
<li>如果 API 函数会创建新的 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Operation"><code>tf.Operation</code></a> 或返回新的 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a>，则会接受可选 <code>name</code> 参数。例如，<code>tf.constant(42.0, name=&quot;answer&quot;)</code> 会创建一个新的 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Operation"><code>tf.Operation</code></a>（名为 <code>&quot;answer&quot;</code>）并返回一个 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a>（名为 <code>&quot;answer:0&quot;</code>）。如果默认图已包含名为 <code>&quot;answer&quot;</code> 的操作，则 TensorFlow 会在名称上附加 <code>&quot;_1&quot;</code>、<code>&quot;_2&quot;</code> 等字符，以便让名称具有唯一性。</li>
<li>借助 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/name_scope"><code>tf.name_scope</code></a> 函数，可以向在特定上下文中创建的所有操作添加名称作用域前缀。当前名称作用域前缀是一个用 <code>&quot;/&quot;</code> 分隔的名称列表，其中包含所有活跃 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/name_scope"><code>tf.name_scope</code></a> 上下文管理器的名称。如果某个名称作用域已在当前上下文中被占用，TensorFlow 将在该作用域上附加 <code>&quot;_1&quot;</code>、<code>&quot;_2&quot;</code> 等字符。例如：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">e_0 = tf.constant(<span class="number">0</span>, name=<span class="string">&quot;e&quot;</span>)</span><br><span class="line">print(e_0)</span><br><span class="line">e_1 = tf.constant(<span class="number">2</span>, name=<span class="string">&quot;e&quot;</span>)</span><br><span class="line">print(e_1)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&quot;outer&quot;</span>):  <span class="comment"># 在命名空间 outer 下创建常量</span></span><br><span class="line">    e_2 = tf.constant(<span class="number">2</span>, name=<span class="string">&quot;e&quot;</span>)</span><br><span class="line">    print(e_2)</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&quot;inner&quot;</span>):  <span class="comment"># 在命名空间 inter 下创建常量</span></span><br><span class="line">        e_3 = tf.constant(<span class="number">3</span>, name=<span class="string">&quot;e&quot;</span>)</span><br><span class="line">        print(e_3)</span><br><span class="line">        e_4 = tf.constant(<span class="number">4</span>, name=<span class="string">&quot;e&quot;</span>)</span><br><span class="line">        print(e_4)</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&quot;inner&quot;</span>):</span><br><span class="line">        e_5 = tf.constant(<span class="number">5</span>, name=<span class="string">&quot;e&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h3><p>我们现在再来看上面所述的加法运算例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(<span class="number">3.0</span>, name=<span class="string">&#x27;a&#x27;</span>)  <span class="comment"># 创建一个常量 a</span></span><br><span class="line">b = tf.constant(<span class="number">4.0</span>, name=<span class="string">&#x27;b&#x27;</span>)  <span class="comment"># 创建一个常量 b</span></span><br><span class="line">c = a + b  <span class="comment"># 创建一个操作 a+b</span></span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>

<p>上面我们说到 a，b，c 只是我们在数据流图中所构建的节点而已，所以我们直接对其进行打印，并不能直接打印出其值。在 TensorFlow 中，需要创建会话才能进行运算，并打印出结果。在TensorFlow 中，创建会话的语句为 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Session"><code>tf.Session</code></a> ，使用会话执行数据流图的计算为 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Session#run"><code>tf.Session.run</code></a> 。下面我们创建一个会话。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(a))</span><br><span class="line">print(sess.run(b))</span><br><span class="line">print(sess.run(c))</span><br></pre></td></tr></table></figure>

<p>由上面的结果可知，输出的结果与我们预想的一致。由于 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Session"><code>tf.Session</code></a> 拥有物理资源（例如 GPU 和网络连接），因此通常（在 <code>with</code> 代码块中）用作上下文管理器，并在你退出代码块时自动关闭会话。当然，你也可以在不使用 <code>with</code> 代码块的情况下创建会话，但应在完成会话时明确调用 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Session#close"><code>tf.Session.close</code></a> 以便释放资源。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(a))</span><br><span class="line">    print(sess.run(b))</span><br><span class="line">    print(sess.run(c))</span><br></pre></td></tr></table></figure>

<p>在实际使用中，<a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Session#run"><code>tf.Session.run</code></a> 也可以选择接受 <strong>feed 字典</strong>，该字典是从 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 对象（通常是 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/placeholder"><code>tf.placeholder</code></a> 张量）到在执行时会替换这些张量的值，通常是 Python 标量、列表或 NumPy 数组的映射。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.int32, shape=[<span class="number">3</span>])  <span class="comment"># 创建一个占位符</span></span><br><span class="line">y = tf.square(x)  <span class="comment"># 创建一个操作，对 x 取平方得到 y</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    feed = &#123;x: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;    <span class="comment"># 运行时，给占位符喂的值</span></span><br><span class="line">    print(sess.run(y, feed_dict=feed))</span><br><span class="line">    print(sess.run(y, &#123;x: [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]&#125;))</span><br></pre></td></tr></table></figure>

<p>这里需要注意的是使用 <code>x = tf.placeholder(tf.int32, shape=[ 3])</code> 创建占位符表示的是在创建计算图时，x 没有被赋予实际的值，而在 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Session#close"><code>tf.Session.close</code></a> 运行计算图时需要对其传入数据，传入数据的方法采用上面所述的<strong>字典</strong>形式。</p>
<h3 id="张量-Tensor"><a href="#张量-Tensor" class="headerlink" title="张量 Tensor"></a>张量 Tensor</h3><p>在 TensorFlow 中，其基本的数据结构是张量（Tensor），其是对矢量和矩阵向潜在的更高维度的泛化。TensorFlow 在内部将张量表示为基本数据类型的 n 维数组，即我们通常所说的多维数组。</p>
<p>在 TensorFlow 中，张量被操作和传递的主要对象是 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a>，其具有以下属性：</p>
<ul>
<li>数据类型（dtype）：指的是张量的数据类型，例如 <code>float32</code>、<code>int32</code> 或 <code>string</code>等；</li>
<li>形状（shape）：指的是张量的维度以及每个维度的大小，例如三维的张量：(4,2,6)。</li>
<li>名字（name）：指的是张量在计算图中的命名。</li>
</ul>
<p>在 TensorFlow 中常用的主要有四种类型的张量，分别如下：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Variable"><code>tf.Variable</code></a> 变量，其值可以在训练中被改变</li>
<li><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/constant"><code>tf.constant</code></a>常量，其值可以在训练中不可改变</li>
<li><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/placeholder"><code>tf.placeholder</code></a>占位符常量，在运行会话时，其值需要给定</li>
<li><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/SparseTensor"><code>tf.SparseTensor</code></a>常量，稀疏张量</li>
</ul>
<p>上面所列的四种类型的张量中，只有 tf.Variable 是可变张量，其他张量均不可改变。</p>
<h4 id="张量的秩"><a href="#张量的秩" class="headerlink" title="张量的秩"></a>张量的秩</h4><p><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 对象的阶是它本身的维数。阶的同义词包括：秩、等级或 n 维。请注意，TensorFlow 中的阶与数学中矩阵的阶并不是同一个概念。如下表所示，TensorFlow 中的每个阶都对应一个不同的数学实例：</p>
<table>
<thead>
<tr>
<th align="right">阶</th>
<th align="right">数学实例</th>
</tr>
</thead>
<tbody><tr>
<td align="right">0</td>
<td align="right">标量（只有大小）</td>
</tr>
<tr>
<td align="right">1</td>
<td align="right">矢量（大小和方向）</td>
</tr>
<tr>
<td align="right">2</td>
<td align="right">矩阵（数据表）</td>
</tr>
<tr>
<td align="right">3</td>
<td align="right">3 阶张量（数据立体）</td>
</tr>
<tr>
<td align="right">n</td>
<td align="right">n 阶张量（自行想象）</td>
</tr>
</tbody></table>
<p>以下演示了创建 0 阶变量的过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mammal = tf.Variable(<span class="string">&quot;Elephant&quot;</span>, tf.string)</span><br><span class="line">mammal</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Variable &#39;Variable:0&#39; shape&#x3D;() dtype&#x3D;string_ref&gt;</span><br></pre></td></tr></table></figure>

<p>要创建 1 阶 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 对象，可以传递一个项目列表作为初始值。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cool_numbers = tf.Variable([<span class="number">3.14159</span>, <span class="number">2.71828</span>], tf.float32)</span><br><span class="line">cool_numbers</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Variable &#39;Variable_1:0&#39; shape&#x3D;(2,) dtype&#x3D;float32_ref&gt;</span><br></pre></td></tr></table></figure>

<p>2 阶 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 对象至少包含一行和一列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">squarish_squares = tf.Variable([[<span class="number">4</span>, <span class="number">9</span>], [<span class="number">16</span>, <span class="number">25</span>]], tf.int32)</span><br><span class="line">squarish_squares</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Variable &#39;Variable_2:0&#39; shape&#x3D;(2, 2) dtype&#x3D;int32_ref&gt;</span><br></pre></td></tr></table></figure>

<p>同样，更高阶的张量由一个 n 维数组组成。例如，在图像处理过程中，会使用许多 4 阶张量，维度对应批次大小、图像宽度、图像高度和颜色通道。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my_image = tf.zeros([<span class="number">10</span>, <span class="number">299</span>, <span class="number">299</span>, <span class="number">3</span>])</span><br><span class="line">my_image</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor &#39;zeros:0&#39; shape&#x3D;(10, 299, 299, 3) dtype&#x3D;float32&gt;</span><br></pre></td></tr></table></figure>

<p>要确定 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 对象的阶，需调用 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/rank"><code>tf.rank</code></a> 方法。例如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r = tf.rank(my_image)</span><br><span class="line">r</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor &#39;Rank:0&#39; shape&#x3D;() dtype&#x3D;int32&gt;</span><br></pre></td></tr></table></figure>

<p>同样使用会话运行 r 才能得到其值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(r))</span><br></pre></td></tr></table></figure>

<h4 id="张量的切片"><a href="#张量的切片" class="headerlink" title="张量的切片"></a>张量的切片</h4><p>由于 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 是 n 维单元数组，因此要访问 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 中的某一单元或元素，需要指定 n 个索引，这与 NumPy 是一致的。对于 2 阶 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a>，传递两个数字会如预期般返回一个标量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">my_matrix = tf.constant([[<span class="number">4</span>, <span class="number">9</span>], [<span class="number">16</span>, <span class="number">25</span>]], tf.int32)</span><br><span class="line">my_scalar = my_matrix[<span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(my_scalar))</span><br></pre></td></tr></table></figure>

<p>从上面运行的结果可以知道，TensorFlow 中张量的切片与 NumPy 中数组的类似。只不过在 TensorFlow 中，任何运算都会看成是一个计算图，所以需要建立会话运行计算图，从而得到计算结果。</p>
<h4 id="张量的形状"><a href="#张量的形状" class="headerlink" title="张量的形状"></a>张量的形状</h4><p>张量的形状是每个维度中元素的数量。TensorFlow 在图的构建过程中自动推理形状。这些推理的形状可能具有已知或未知的阶。如果阶已知，则每个维度的大小可能已知或未知。</p>
<p>TensorFlow 文件编制中通过三种符号约定来描述张量维度：阶，形状和维数。下表阐述了三者如何相互关联：</p>
<table>
<thead>
<tr>
<th align="right">阶</th>
<th align="right">形状</th>
<th align="right">维数</th>
<th align="right">示例</th>
</tr>
</thead>
<tbody><tr>
<td align="right">0</td>
<td align="right">[]</td>
<td align="right">0-D</td>
<td align="right">0 维张量。标量。</td>
</tr>
<tr>
<td align="right">1</td>
<td align="right">[D0]</td>
<td align="right">1-D</td>
<td align="right">形状为 [5] 的 1 维张量。</td>
</tr>
<tr>
<td align="right">2</td>
<td align="right">[D0, D1]</td>
<td align="right">2-D</td>
<td align="right">形状为 [3, 4] 的 2 维张量。</td>
</tr>
<tr>
<td align="right">3</td>
<td align="right">[D0, D1, D2]</td>
<td align="right">3-D</td>
<td align="right">形状为 [1, 4, 3] 的 3 维张量。</td>
</tr>
<tr>
<td align="right">n</td>
<td align="right">[D0, D1, … Dn-1]</td>
<td align="right">n 维</td>
<td align="right">形状为 [D0, D1, … Dn-1] 的张量。</td>
</tr>
</tbody></table>
<p>可以通过两种方法获取 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 的形状。在构建图的时候，询问有关张量形状的已知信息通常很有帮助。可以通过查看 <code>shape</code> 属性（属于 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 对象）获取这些信息。该方法会返回一个 <code>TensorShape</code> 对象，这样可以方便地表示部分指定的形状，因为在构建图的时候，并不是所有形状都完全已知。</p>
<p>也可以获取一个将在运行时表示另一个 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 的完全指定形状的 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a>。为此，可以调用 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/shape"><code>tf.shape</code></a> 操作。如此一来，可以构建一个图，通过构建其他取决于输入 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 的动态形状的张量来控制张量的形状。例如，以下代码展示了如何创建大小与给定矩阵中的列数相同的零矢量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">my_matrix = tf.constant([[<span class="number">4</span>, <span class="number">9</span>], [<span class="number">16</span>, <span class="number">25</span>]], tf.int32)  <span class="comment"># 创建一个常量</span></span><br><span class="line">print(my_matrix)</span><br><span class="line">zeros = tf.zeros(my_matrix.shape[<span class="number">0</span>])  <span class="comment"># 创建一个全为 0 的常量</span></span><br><span class="line">print(zeros)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(zeros))   <span class="comment"># 打印 zeros 矩阵</span></span><br></pre></td></tr></table></figure>

<p>张量的元素数量是其所有形状大小的乘积。由于通常有许多不同的形状具有相同数量的元素，因此如果能够改变 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 的形状并使其元素固定不变通常会很方便。为此，可以使用 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/reshape"><code>tf.reshape</code></a>。以下示例演示如何重构张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">rank_three_tensor = tf.ones([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])  <span class="comment"># 创建一个全为 1 的矩阵常量</span></span><br><span class="line">print(rank_three_tensor)</span><br><span class="line">matrix = tf.reshape(rank_three_tensor, [<span class="number">6</span>, <span class="number">10</span>])   <span class="comment"># 改变形状</span></span><br><span class="line">print(matrix)</span><br><span class="line">matrixB = tf.reshape(matrix, [<span class="number">3</span>, <span class="number">-1</span>])   <span class="comment"># -1 表示系统自行判断大小</span></span><br><span class="line">print(matrixB)</span><br><span class="line">matrixAlt = tf.reshape(matrixB, [<span class="number">4</span>, <span class="number">3</span>, <span class="number">-1</span>])</span><br><span class="line">print(matrixAlt)</span><br><span class="line">yet_another = tf.reshape(matrixAlt, [<span class="number">3</span>, <span class="number">2</span>, <span class="number">-1</span>])</span><br><span class="line">print(yet_another)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;输出：</span></span><br><span class="line"><span class="string">Tensor(&quot;ones:0&quot;, shape=(3, 4, 5), dtype=float32)</span></span><br><span class="line"><span class="string">Tensor(&quot;Reshape:0&quot;, shape=(6, 10), dtype=float32)</span></span><br><span class="line"><span class="string">Tensor(&quot;Reshape_1:0&quot;, shape=(3, 20), dtype=float32)</span></span><br><span class="line"><span class="string">Tensor(&quot;Reshape_2:0&quot;, shape=(4, 3, 5), dtype=float32)</span></span><br><span class="line"><span class="string">Tensor(&quot;Reshape_3:0&quot;, shape=(3, 2, 10), dtype=float32)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h4 id="张量的数据类型"><a href="#张量的数据类型" class="headerlink" title="张量的数据类型"></a>张量的数据类型</h4><p>除维度外，张量还具有数据类型。如需数据类型的完整列表，请参阅 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/DType"><code>tf.DType</code></a> 页面。一个 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 只能有一种数据类型。但是，可以将 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 从一种数据类型转型为另一种，这需要通过 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/cast"><code>tf.cast</code></a> 函数来执行，例如下面例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">list0 = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=tf.int32)</span><br><span class="line">print(list0)</span><br><span class="line">float_tensor = tf.cast(list0, dtype=tf.float32)</span><br><span class="line">print(float_tensor)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;输出：</span></span><br><span class="line"><span class="string">Tensor(&quot;Const_9:0&quot;, shape=(3,), dtype=int32)</span></span><br><span class="line"><span class="string">Tensor(&quot;Cast:0&quot;, shape=(3,), dtype=float32)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>要检查 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 的数据类型，可以使用 <code>Tensor.dtype</code> 属性。用 Python 对象创建 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a> 时，可以选择指定数据类型。如果不指定数据类型，TensorFlow 会自动选择一个合适的数据类型。TensorFlow 会将 Python 整数转型为 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/int32"><code>tf.int32</code></a>，并将 Python 浮点数转型为 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/float32"><code>tf.float32</code></a>。此外，TensorFlow 使用 Numpy 在转换至数组时使用的相同规则。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>占位符适用于简单的实验，而 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/data"> <em>数据集</em></a> 是将数据流传输到模型的首选方法。要从数据集中获取可运行的 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a>，必须先将其转换成 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/data/Iterator"><code>tf.data.Iterator</code></a>，然后调用迭代器的 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/data/Iterator#get_next"><code>get_next</code></a> 方法。创建迭代器的最简单的方式是采用 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/data/Dataset#make_one_shot_iterator"><code>make_one_shot_iterator</code></a> 方法。例如，在下面的代码中，<code>next_item</code> 张量将在每次 <code>run</code> 调用时从 <code>my_data</code> 阵列返回一行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">my_data = [     <span class="comment"># 初始化一个二维数组</span></span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, ],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">3</span>, ],</span><br><span class="line">    [<span class="number">4</span>, <span class="number">5</span>, ],</span><br><span class="line">    [<span class="number">6</span>, <span class="number">7</span>, ],</span><br><span class="line">]</span><br><span class="line">slices = tf.data.Dataset.from_tensor_slices(my_data)  <span class="comment"># 构建数据集</span></span><br><span class="line">next_item = slices.make_one_shot_iterator().get_next()  <span class="comment"># 定义一个批次取的迭代器</span></span><br></pre></td></tr></table></figure>

<p>到达数据流末端时，<code>Dataset</code> 会抛出 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/errors/OutOfRangeError"><code>OutOfRangeError</code></a>。例如，下面的代码会一直读取 <code>next_item</code>，直到没有数据可读：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">try</span>:     <span class="comment"># 尝试去取数据</span></span><br><span class="line">            print(sess.run(next_item))</span><br><span class="line">        <span class="keyword">except</span> tf.errors.OutOfRangeError:  <span class="comment"># 取完时报错，结束运行</span></span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>如果 Dataset 依赖于有状态操作，即每个批次处理都依赖之前的批次数据或者中间结果来计算当前批次的数据。因此，需要在使用迭代器之前先初始化它，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">r = tf.random_normal([<span class="number">10</span>, <span class="number">3</span>])</span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(r)</span><br><span class="line">iterator = dataset.make_initializable_iterator()</span><br><span class="line">next_row = iterator.get_next()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(iterator.initializer)  <span class="comment"># 初始化迭代器</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">try</span>:   <span class="comment"># 尝试去取数据</span></span><br><span class="line">            print(sess.run(next_row))</span><br><span class="line">        <span class="keyword">except</span> tf.errors.OutOfRangeError:  <span class="comment"># 取完时报错，结束运行</span></span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>要详细了解数据集和迭代器，请参阅 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/guide/datasets"> <em>导入数据</em></a>。</p>
<h3 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h3><p>TensorFlow 主要是用来搭建深度学习模型的，因此其通过各种各样的 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/layers"> <em>层</em></a> 来创建神经网络的每一层。</p>
<p>层将变量和作用于它们的操作打包在一起。例如，<a target="_blank" rel="noopener" href="https://developers.google.cn/machine-learning/glossary/#fully_connected_layer"> <em>密集连接层</em></a> 会对每个输出对应的所有输入执行加权和，并应用 <a target="_blank" rel="noopener" href="https://developers.google.cn/machine-learning/glossary/#activation_function"> <em>激活函数</em></a> （可选）。连接权重和偏差由层对象管理。</p>
<h4 id="创建层"><a href="#创建层" class="headerlink" title="创建层"></a>创建层</h4><p>下面的代码会创建一个 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/layers/Dense"><code>Dense</code></a> 层，该层会接受一批输入矢量，并为每个矢量生成一个输出值。要将层应用于输入值，请将该层当做函数来调用。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">3</span>])  <span class="comment"># 创建一个占位符</span></span><br><span class="line">linear_model = tf.layers.Dense(units=<span class="number">1</span>)     <span class="comment"># 创建一个 Dense 层</span></span><br><span class="line">y = linear_model(x)                 <span class="comment"># 将 x 输入到 Dense 层，然后得到 y</span></span><br></pre></td></tr></table></figure>

<p>层会检查其输入数据，以确定其内部变量的大小。因此，必须在这里设置 x 占位符的形状，以便层构建正确大小的权重矩阵。现在已经定义了输出值 y 的计算，在运行计算之前，还需要处理一个细节。</p>
<h4 id="初始化层"><a href="#初始化层" class="headerlink" title="初始化层"></a>初始化层</h4><p>层包含的变量必须先初始化，然后才能使用。尽管可以单独初始化各个变量，但也可以轻松地初始化一个 TensorFlow 图中的所有变量，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()   <span class="comment"># 定义一个全局初始化操作</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)           <span class="comment"># 全局初始化操作</span></span><br></pre></td></tr></table></figure>

<h4 id="执行层"><a href="#执行层" class="headerlink" title="执行层"></a>执行层</h4><p>我们现在已经完成了层的初始化，可以像处理任何其他张量一样评估 linear_model 的输出张量了。例如，下面的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(y, &#123;x: [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]&#125;))</span><br></pre></td></tr></table></figure>

<h4 id="层函数的快捷方式"><a href="#层函数的快捷方式" class="headerlink" title="层函数的快捷方式"></a>层函数的快捷方式</h4><p>对于每个层类（如 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/layers/Dense"><code>tf.layers.Dense</code></a>)，TensorFlow 还提供了一个快捷函数（如 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/layers/dense"><code>tf.layers.dense</code></a>）。两者唯一的区别是快捷函数版本是在单次调用中创建和运行层。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">3</span>])  <span class="comment"># 创建一个占位符 x</span></span><br><span class="line">y = tf.layers.dense(x, units=<span class="number">1</span>)    <span class="comment"># 创建一个dense层，输入 x 得到 y</span></span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()  <span class="comment"># 定义一个全局初始化操作</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(y, &#123;x: [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]&#125;))</span><br></pre></td></tr></table></figure>

<p>尽管这种方法很方便，但无法访问 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/layers/Layer"><code>tf.layers.Layer</code></a> 对象。这会让自省和调试变得更加困难，并且无法重复使用相应的层。</p>
<h3 id="回归模型"><a href="#回归模型" class="headerlink" title="回归模型"></a>回归模型</h3><p>现在已经了解 TensorFlow 核心部分的基础知识了，我们来手动训练一个小型回归模型吧。</p>
<p>我们首先来定义一些输入值 x，以及每个输入值对应的真实输出值 y_true：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>], [<span class="number">4</span>]], dtype=tf.float32)</span><br><span class="line">y_true = tf.constant([[<span class="number">0</span>], [<span class="number">-1</span>], [<span class="number">-2</span>], [<span class="number">-3</span>]], dtype=tf.float32)</span><br></pre></td></tr></table></figure>

<p>接下来，建立一个简单的线性模型，其输出值只有 1 个：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">linear_model = tf.layers.Dense(units=<span class="number">1</span>)</span><br><span class="line">y_pred = linear_model(x)</span><br></pre></td></tr></table></figure>

<p>你可以如下评估预测值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(y_pred))</span><br></pre></td></tr></table></figure>

<p>该模型尚未接受训练，因此四个 “预测” 值并不理想。因为网络层的权重值是随机初始化的，所以多次运行的输出应该有所不同：</p>
<p>要优化模型，首先需要定义损失函数。我们将使用均方误差，这是回归问题的标准损失。</p>
<p>虽然你可以使用较低级别的数学运算手动定义，但 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/losses"><code>tf.losses</code></a> 模块提供了一系列常用的损失函数。你可以使用它来计算均方误差，具体操作如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(loss))</span><br></pre></td></tr></table></figure>

<p>TensorFlow 提供了执行标准优化算法的 <a target="_blank" rel="noopener" href="https://developers.google.cn/machine-learning/glossary/#optimizer"> <em>优化器</em></a> 。这些优化器被实现为 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/train/Optimizer"><code>tf.train.Optimizer</code></a> 的子类。它们会逐渐改变每个变量，以便将损失最小化。最简单的优化算法是 <a target="_blank" rel="noopener" href="https://developers.google.cn/machine-learning/glossary/#gradient_descent"> <em>梯度下降法</em></a> ，由 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/train/GradientDescentOptimizer"><code>tf.train.GradientDescentOptimizer</code></a> 实现。它会根据损失相对于变量的导数大小来修改各个变量。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)   <span class="comment"># 创建优化器</span></span><br><span class="line">train = optimizer.minimize(loss)    <span class="comment"># 优化损失</span></span><br></pre></td></tr></table></figure>

<p>该代码构建了优化所需的所有图组件，并返回一个训练指令。该训练指令在运行时会更新图中的变量。你可以按以下方式运行该指令：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)  <span class="comment"># 全局初始化</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        _, loss_value = sess.run((train, loss))</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            print(loss_value)</span><br></pre></td></tr></table></figure>

<p>由上面的输出结果可以知道，随着迭代次数的增加，模型的损失函数值在不断的下降。</p>
<h3 id="模型的保存与恢复"><a href="#模型的保存与恢复" class="headerlink" title="模型的保存与恢复"></a>模型的保存与恢复</h3><p>一般情况下，当我们训练完模型之后，需要把训练结果保存下来，以便测试的时候使用。在 TensorFlow 中，通过 <code>tf.train.Saver()</code> 来保存图模型。</p>
<p>创建 <code>Saver</code>来管理模型中的所有变量。例如，以下代码段展示了如何调用 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/train/Saver#save"><code>tf.train.Saver.save</code></a> 方法以将变量保存到检查点文件中。因为前面所构建的变量都存于一个图中，这里为了防止变量冲突，新建另一个图来运行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">g_2 = tf.Graph()    <span class="comment"># 重新定义一个图</span></span><br><span class="line"><span class="keyword">with</span> g_2.as_default():  <span class="comment"># 在 g_2 图下创建节点</span></span><br><span class="line">    x = tf.constant([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>], [<span class="number">4</span>]], dtype=tf.float32)</span><br><span class="line">    linear_model = tf.layers.Dense(units=<span class="number">1</span>,name=<span class="string">&#x27;g_2&#x27;</span>)</span><br><span class="line">    y_pred = linear_model(x)</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    <span class="keyword">with</span> tf.Session(graph=g_2) <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(init)</span><br><span class="line">        print(sess.run(y_pred))</span><br><span class="line">        save_path = saver.save(sess, <span class="string">&quot;./temp/model.ckpt&quot;</span>)</span><br><span class="line">        print(<span class="string">&quot;保存成功&quot;</span>)</span><br><span class="line">        print(<span class="string">&quot;Model saved in path: %s&quot;</span> % save_path)</span><br></pre></td></tr></table></figure>

<p>从上面的运行结果可以看出，我们已经成功保存了模型，我们可以通过下面命令来查看所保存的模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!tree</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── lab.ipynb</span><br><span class="line">└── temp</span><br><span class="line">    ├── checkpoint</span><br><span class="line">    ├── model.ckpt.data-00000-of-00001</span><br><span class="line">    ├── model.ckpt.index</span><br><span class="line">    └── model.ckpt.meta</span><br><span class="line"></span><br><span class="line">1 directory, 5 files</span><br></pre></td></tr></table></figure>

<p>可以看到我们所保存的模型在文件夹 temp 下方，总共含有四个文件。.meta 文件表示模型的图结构，.data-00000-of-00001 和 .index 文件表示模型的权重文件；checkpoint 表示检查点文件。</p>
<p>现在来将模型读取出来。这里需要注意的是 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/train/Saver"><code>tf.train.Saver</code></a> 对象不仅将变量保存到检查点文件中，还将恢复变量。当恢复变量时，不必事先将其初始化。例如，以下代码段展示了如何调用 <a target="_blank" rel="noopener" href="https://tensorflow.google.cn/api_docs/python/tf/train/Saver#restore"><code>tf.train.Saver.restore</code></a> 方法以从检查点文件中恢复变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">g_2 = tf.Graph()    </span><br><span class="line"><span class="keyword">with</span> g_2.as_default():  </span><br><span class="line">    x = tf.constant([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>], [<span class="number">4</span>]], dtype=tf.float32)</span><br><span class="line">    linear_model = tf.layers.Dense(units=<span class="number">1</span>,name=<span class="string">&#x27;g_2&#x27;</span>)</span><br><span class="line">    y_pred = linear_model(x)</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    <span class="keyword">with</span> tf.Session(graph=g_2) <span class="keyword">as</span> sess:</span><br><span class="line">        save_path = saver.restore(sess, <span class="string">&quot;./temp/model.ckpt&quot;</span>)</span><br><span class="line">        print(sess.run(y_pred))</span><br><span class="line">        print(<span class="string">&quot;读取成功&quot;</span>)</span><br><span class="line">        print(<span class="string">&quot;Model saved in path: %s&quot;</span> % save_path)</span><br></pre></td></tr></table></figure>

<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过以上学习，我们主要了解了 TensorFlow 的基本概念，如数据量流图，会话，张量等，并动手使用 TensorFlow 来实现一个简单的线性回归例子。此外还讲解了模型的保存与恢复。相信你此时已经对 TensorFlow 有一个初步的了解。</p>
<hr>
<img src="/2020/04/09/tensorflow-ji-ben-gai-nian-ji-gou-jian/hi.png" class=""></div><div class="post-copyright"><blockquote><p>原文作者: 贺同学</p><p>原文链接: <a href="http://clarkhedi.github.io/2020/04/09/tensorflow-ji-ben-gai-nian-ji-gou-jian/">http://clarkhedi.github.io/2020/04/09/tensorflow-ji-ben-gai-nian-ji-gou-jian/</a></p><p>版权声明: 转载请注明出处(必须保留原文作者署名原文链接)</p></blockquote></div><div class="tags"><a href="/tags/Python/">Python</a><a href="/tags/TensorFlow/">TensorFlow</a></div><div class="post-share"><div class="social-share"><span>分享到:</span></div><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到:</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2020/04/09/tensorflow-keras-gao-jie-ying-yong/" class="pre">TensorFlow Keras 高阶应用</a><a href="/2020/04/02/che-pai-shi-bie-lpr-qi-zi-fu-shi-bie/" class="next">车牌识别LPR（七）-- 字符识别</a></div><div id="comments"><div id="container"><script type="text/javascript" src="https://unpkg.com/gitalk/dist/gitalk.min.js?v=2.0.5"></script><script type="text/javascript" src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js?v=2.0.5"></script><script>var gitalk = new Gitalk({
  clientID: 'd6d4fb51a5284e4df5fd',
  clientSecret: '80ba8b4586bfce6df672e814df089b31e49a5549',
  repo: 'Gitalk',
  owner: 'clarkhedi',
  admin: ['clarkhedi'],
  id: md5(window.location.pathname),
  distractionFreeMode: false,
  language: 'zh-CN',
  pagerDirection: 'last'
})
gitalk.render('container')</script></div></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E4%BD%93%E4%BB%8B%E7%BB%8D"><span class="toc-text">总体介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="toc-text">知识点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TensorFlow-%E4%BB%8B%E7%BB%8D"><span class="toc-text">TensorFlow 介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%B5%81%E5%9B%BE"><span class="toc-text">数据流图</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA-tf-Graph"><span class="toc-text">构建 tf.Graph</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4"><span class="toc-text">命名空间</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%9A%E8%AF%9D"><span class="toc-text">会话</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F-Tensor"><span class="toc-text">张量 Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E7%A7%A9"><span class="toc-text">张量的秩</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E5%88%87%E7%89%87"><span class="toc-text">张量的切片</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E5%BD%A2%E7%8A%B6"><span class="toc-text">张量的形状</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-text">张量的数据类型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E5%B1%82"><span class="toc-text">网络层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%B1%82"><span class="toc-text">创建层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%B1%82"><span class="toc-text">初始化层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E5%B1%82"><span class="toc-text">执行层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B1%82%E5%87%BD%E6%95%B0%E7%9A%84%E5%BF%AB%E6%8D%B7%E6%96%B9%E5%BC%8F"><span class="toc-text">层函数的快捷方式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-text">回归模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E6%81%A2%E5%A4%8D"><span class="toc-text">模型的保存与恢复</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/11/18/hexo-start/">Hexo Start</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/16/ge-xing-hua-ding-zhi-ni-de-github-zhu-ye/">2021年，教程 | 个性化定制你的 GitHub 主页</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/07/you-mei-de-sublime-text/">优美的Sublime Text</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/08/pyqt5-xue-xi-bi-ji-yi/">PyQt5学习笔记（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/17/kaiming-he-chu-shi-hua-de-xue-xi/">Kaiming He初始化的学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/autograd-yu-luo-ji-hui-gui/">autograd与逻辑回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/ji-suan-tu-yu-dong-tai-tu-ji-zhi/">计算图与动态图机制</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/zhang-liang-cao-zuo-yu-xian-xing-hui-gui/">张量操作与线性回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/tensor-zhang-liang-jie-shao/">Tensor（张量）介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/13/triplet-loss-xue-xi/">triplet-loss学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/GitHub%E5%AD%A6%E4%B9%A0/">GitHub学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo%E5%AD%A6%E4%B9%A0/">Hexo学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Jupyter%E8%AF%AD%E6%B3%95/">Jupyter语法</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux%E5%AD%A6%E4%B9%A0/">Linux学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/OpenCV-Python%E6%89%8B%E8%AE%B0/">OpenCV-Python手记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Python学习笔记</a><span class="category-list-count">8</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/PyQt5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">PyQt5学习笔记</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Pytorch学习笔记</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E7%B1%BB%E6%8A%80%E6%9C%AF/">分类技术</a><span class="category-list-count">14</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E7%B1%BB%E6%8A%80%E6%9C%AF/LBP%E5%AD%A6%E4%B9%A0/">LBP学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E7%B1%BB%E6%8A%80%E6%9C%AF/SVM%E5%85%AB%E8%82%A1/">SVM八股</a><span class="category-list-count">10</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">图像处理学习笔记</a><span class="category-list-count">7</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB/">车牌识别</a><span class="category-list-count">7</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AE%9E%E9%AA%8C%E6%A5%BC%E5%AD%A6%E4%B9%A0/">实验楼学习</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E8%B8%A9%E5%9D%91/">环境搭建踩坑</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/">英语学习</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">论文学习笔记</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/CLBP/" style="font-size: 15px;">CLBP</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">论文学习</a> <a href="/tags/vim/" style="font-size: 15px;">vim</a> <a href="/tags/OpenCV/" style="font-size: 15px;">OpenCV</a> <a href="/tags/PyQt5/" style="font-size: 15px;">PyQt5</a> <a href="/tags/GUI/" style="font-size: 15px;">GUI</a> <a href="/tags/%E8%AE%B0%E5%8D%95%E8%AF%8D/" style="font-size: 15px;">记单词</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/GitHub/" style="font-size: 15px;">GitHub</a> <a href="/tags/Resume/" style="font-size: 15px;">Resume</a> <a href="/tags/LBP/" style="font-size: 15px;">LBP</a> <a href="/tags/LPR/" style="font-size: 15px;">LPR</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/">2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/">2021</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">36</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">5</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> 友情链接</i></div><ul></ul><a href="https://github.com/chaooo/hexo-theme-BlueLake" title="BlueLake主题" target="_blank">BlueLake主题</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247484443&amp;idx=1&amp;sn=7110e42ef9e95a8c16064dde5b897960&amp;chksm=e870d556df075c4053a90d207c078e5fe965b5f9004de7f34dee09fccf1a37977aa3860b3a96&amp;mpshare=1&amp;scene=23&amp;srcid=0428XfRkvl0HsaHMU71k04dq#rd" title="重磅|完备的AI学习路线,最详细的资源整理！" target="_blank">重磅|完备的AI学习路线,最详细的资源整理！</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzI5NDY1MjQzNA==&amp;mid=2247489096&amp;idx=1&amp;sn=ca2e1d72c9decd021c0365cc9e93a539&amp;chksm=ec5ec935db2940237482c5581b7a45f2bd50b138efa6488ed9b942c182486a4e74131e9af523&amp;mpshare=1&amp;scene=23&amp;srcid=#rd" title="推荐|机器学习入门方法和资料合集" target="_blank">推荐|机器学习入门方法和资料合集</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzU4NTY4Mzg1Mw==&amp;mid=100000497&amp;idx=1&amp;sn=96cc795f1c22f7da2f3260d4ea364ff3&amp;chksm=7d8784134af00d05e41da36475bca417851535ed9710c7bd44ae7365cf40857015d922671c95&amp;mpshare=1&amp;scene=23&amp;srcid=04138MekLgFi3vNAz2E2rTrv#rd" title="良心推荐：机器学习入门资料汇总及学习建议（2018版）" target="_blank">良心推荐：机器学习入门资料汇总及学习建议（2018版）</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&amp;mid=2649032678&amp;idx=1&amp;sn=7ce30241f24dae790f7361173132ee04&amp;chksm=8712b99bb065308d6626ac71d358d386199f5fdcc0d20f48b9ff7f7f84f32892325075413d78&amp;mpshare=1&amp;scene=1&amp;srcid=1213VnRvWgKRhAswA6qj9C6U&amp;sharer_sharetime=1607823235596&amp;sharer_shareid=6f3d2b6e1888cc6674af1cb9d8f856b5&amp;key=94e16379e8fc498f20603adf1c7c1474460f32c861df423924a01860bf66b23e3b54ec715f59513ed57f7be44b91602324165303caa8015279359147bf99dc0ac5c9482bbf323df6415707db1df1dbb44795972834805221b6810c0783bc61213b424afc1f9c747d8eddfe17782e3165ddfff5144eff7867ea82ee34ab2e6e44&amp;ascene=1&amp;uin=MzMyMzAzNzE5OA%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=en&amp;exportkey=A%2Fn4vOWzACQbiXdS3ts1%2BmU%3D&amp;pass_ticket=X0sXcXVfthe9ne%2BkdsNidLJDaUos8pPBg0qSPgQJE8kY3CQmybx%2FB2TVL1w9Glu6&amp;wx_header=0" title="深度学习CV算法工程师从入门到初级面试" target="_blank">深度学习CV算法工程师从入门到初级面试</a></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">贺同学.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a target="_blank" rel="noopener" href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.5"></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="/js/imgview.js?v=2.0.5" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.5" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>
<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.5"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.5"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><link rel="stylesheet" type="text/css" href="https://unpkg.com/gitalk/dist/gitalk.css?v=2.0.5"><title>Kaiming He初始化的学习 | 我中意你23332</title><meta name="generator" content="Hexo 5.2.0"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Kaiming He初始化的学习</h1><a id="logo" href="/.">我中意你23332</a><p class="description">Tomorrow comes never.</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">Kaiming He初始化的学习</h1><div class="post-meta"><a href="/2021/04/17/kaiming-he-chu-shi-hua-de-xue-xi/#comments" class="comment-count"></a><p><span class="date">Apr 17, 2021</span><span><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" class="category">计算机视觉</a><a href="/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="category">论文学习笔记</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><p><strong>在CNN的训练中，权重初始化是一个比较关键的点。好的权重初始化可以让网络的训练过程更加稳定和高效。本文为大家介绍了kaiming初始化以及详细的推导过程，希望可以让大家更好的理解CNN初始化。</strong></p>
<blockquote>
<p>以下文章来源于公众号GiantPandaCV</p>
</blockquote>
<h3 id="1-为什么需要好的权重初始化"><a href="#1-为什么需要好的权重初始化" class="headerlink" title="1. 为什么需要好的权重初始化"></a>1. 为什么需要好的权重初始化</h3><p>网络训练的过程中, 容易出现梯度消失(梯度特别的接近0)和梯度爆炸(梯度特别的大)的情况,导致大部分反向传播得到的梯度不起作用或者起反作用. 研究人员希望能够有一种好的权重初始化方法: 让网络前向传播或者反向传播的时候, 卷积的输出和前传的梯度比较稳定. 合理的方差既保证了数值一定的不同, 又保证了数值一定的稳定.(通过卷积权重的合理初始化, 让计算过程中的数值分布稳定)</p>
<h3 id="2-kaiming初始化的两个方法"><a href="#2-kaiming初始化的两个方法" class="headerlink" title="2. kaiming初始化的两个方法"></a>2. kaiming初始化的两个方法</h3><h4 id="2-1-先来个结论"><a href="#2-1-先来个结论" class="headerlink" title="2.1 先来个结论"></a>2.1 先来个结论</h4><ul>
<li><p>前向传播的时候, 每一层的卷积计算结果的方差为1.</p>
</li>
<li><p>反向传播的时候, 每一层的继续往前传的梯度方差为1(因为每层会有两个梯度的计算, 一个用来更新当前层的权重, 一个继续传播, 用于前面层的梯度的计算.)</p>
</li>
</ul>
<h4 id="2-2-再来个源码"><a href="#2-2-再来个源码" class="headerlink" title="2.2 再来个源码"></a>2.2 再来个源码</h4><p>方差的计算需要两个值：<strong>gain</strong>和<strong>fan</strong>。<strong>gain</strong>值由激活函数决定。<strong>fan</strong>值由权重参数的数量和传播的方向决定。<strong>fan_in</strong>表示前向传播，<strong>fan_out</strong>表示反向传播。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kaiming_normal_</span>(<span class="params">tensor, a=<span class="number">0</span>, mode=<span class="string">&#x27;fan_in&#x27;</span>, nonlinearity=<span class="string">&#x27;leaky_relu&#x27;</span></span>):</span></span><br><span class="line">    fan = _calculate_correct_fan(tensor, mode) </span><br><span class="line">    <span class="comment"># 通过mode判断是前向传播还是反向传播, 生成不同的一个fan值.</span></span><br><span class="line">    gain = calculate_gain(nonlinearity, a)</span><br><span class="line">    <span class="comment"># 通过判断是哪种激活函数生成一个gain值</span></span><br><span class="line">    std = gain / math.sqrt(fan) <span class="comment"># 通过fan值和gain值进行标准差的计算</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">return</span> tensor.normal_(<span class="number">0</span>, std)</span><br></pre></td></tr></table></figure>

<p>下面的代码根据网络设计时<strong>卷积权重的形状</strong>和是前向传播还是反向传播，进行<strong>fan</strong>值的计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_calculate_fan_in_and_fan_out</span>(<span class="params">tensor</span>):</span></span><br><span class="line">    dimensions = tensor.dim() <span class="comment"># 返回的是维度</span></span><br><span class="line">    <span class="keyword">if</span> dimensions &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Fan in and fan out can not be computed for tensor with fewer than 2 dimensions&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> dimensions == <span class="number">2</span>:  <span class="comment"># Linear</span></span><br><span class="line">        fan_in = tensor.size(<span class="number">1</span>) </span><br><span class="line">        fan_out = tensor.size(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        num_input_fmaps = tensor.size(<span class="number">1</span>) <span class="comment"># 卷积的输入通道大小</span></span><br><span class="line">        num_output_fmaps = tensor.size(<span class="number">0</span>) <span class="comment"># 卷积的输出通道大小</span></span><br><span class="line">        receptive_field_size = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> tensor.dim() &gt; <span class="number">2</span>:</span><br><span class="line">            receptive_field_size = tensor[<span class="number">0</span>][<span class="number">0</span>].numel() <span class="comment"># 卷积核的大小:k*k</span></span><br><span class="line">        fan_in = num_input_fmaps * receptive_field_size <span class="comment"># 输入通道数量*卷积核的大小. 用于前向传播</span></span><br><span class="line">        fan_out = num_output_fmaps * receptive_field_size <span class="comment"># 输出通道数量*卷积核的大小. 用于反向传播</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fan_in, fan_out</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_calculate_correct_fan</span>(<span class="params">tensor, mode</span>):</span></span><br><span class="line">    mode = mode.lower()</span><br><span class="line">    valid_modes = [<span class="string">&#x27;fan_in&#x27;</span>, <span class="string">&#x27;fan_out&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> mode <span class="keyword">not</span> <span class="keyword">in</span> valid_modes:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Mode &#123;&#125; not supported, please use one of &#123;&#125;&quot;</span>.<span class="built_in">format</span>(mode, valid_modes))</span><br><span class="line"></span><br><span class="line">    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)</span><br><span class="line">    <span class="keyword">return</span> fan_in <span class="keyword">if</span> mode == <span class="string">&#x27;fan_in&#x27;</span> <span class="keyword">else</span> fan_out</span><br></pre></td></tr></table></figure>

<p>下面是通过不同的激活函数返回一个<strong>gain</strong>值，当然也说明了是recommend。可以自己修改。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_gain</span>(<span class="params">nonlinearity, param=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Return the recommended gain value for the given nonlinearity function.</span></span><br><span class="line"><span class="string">    The values are as follows:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ================= ====================================================</span></span><br><span class="line"><span class="string">    nonlinearity      gain</span></span><br><span class="line"><span class="string">    ================= ====================================================</span></span><br><span class="line"><span class="string">    Linear / Identity :math:`1`</span></span><br><span class="line"><span class="string">    Conv&#123;1,2,3&#125;D      :math:`1`</span></span><br><span class="line"><span class="string">    Sigmoid           :math:`1`</span></span><br><span class="line"><span class="string">    Tanh              :math:`\frac&#123;5&#125;&#123;3&#125;`</span></span><br><span class="line"><span class="string">    ReLU              :math:`\sqrt&#123;2&#125;`</span></span><br><span class="line"><span class="string">    Leaky Relu        :math:`\sqrt&#123;\frac&#123;2&#125;&#123;1 + \text&#123;negative\_slope&#125;^2&#125;&#125;`</span></span><br><span class="line"><span class="string">    ================= ====================================================</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        nonlinearity: the non-linear function (`nn.functional` name)</span></span><br><span class="line"><span class="string">        param: optional parameter for the non-linear function</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; gain = nn.init.calculate_gain(&#x27;leaky_relu&#x27;, 0.2)  # leaky_relu with negative_slope=0.2</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    linear_fns = [<span class="string">&#x27;linear&#x27;</span>, <span class="string">&#x27;conv1d&#x27;</span>, <span class="string">&#x27;conv2d&#x27;</span>, <span class="string">&#x27;conv3d&#x27;</span>, <span class="string">&#x27;conv_transpose1d&#x27;</span>, <span class="string">&#x27;conv_transpose2d&#x27;</span>, <span class="string">&#x27;conv_transpose3d&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> nonlinearity <span class="keyword">in</span> linear_fns <span class="keyword">or</span> nonlinearity == <span class="string">&#x27;sigmoid&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> nonlinearity == <span class="string">&#x27;tanh&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">5.0</span> / <span class="number">3</span></span><br><span class="line">    <span class="keyword">elif</span> nonlinearity == <span class="string">&#x27;relu&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> math.sqrt(<span class="number">2.0</span>)</span><br><span class="line">    <span class="keyword">elif</span> nonlinearity == <span class="string">&#x27;leaky_relu&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> param <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            negative_slope = <span class="number">0.01</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(param, <span class="built_in">bool</span>) <span class="keyword">and</span> <span class="built_in">isinstance</span>(param, <span class="built_in">int</span>) <span class="keyword">or</span> <span class="built_in">isinstance</span>(param, <span class="built_in">float</span>):</span><br><span class="line">            <span class="comment"># True/False are instances of int, hence check above</span></span><br><span class="line">            negative_slope = param</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;negative_slope &#123;&#125; not a valid number&quot;</span>.<span class="built_in">format</span>(param))</span><br><span class="line">        <span class="keyword">return</span> math.sqrt(<span class="number">2.0</span> / (<span class="number">1</span> + negative_slope ** <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Unsupported nonlinearity &#123;&#125;&quot;</span>.<span class="built_in">format</span>(nonlinearity))</span><br></pre></td></tr></table></figure>

<p>下面是kaiming初始化均匀分布的计算。为啥还有个均匀分布？<strong>权重初始化推导的只是一个方差，并没有限定是正态分布</strong>，均匀分布也是有方差的，并且均值为0的时候，可以通过方差算出均匀分布的最小值和最大值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kaiming_uniform_</span>(<span class="params">tensor, a=<span class="number">0</span>, mode=<span class="string">&#x27;fan_in&#x27;</span>, nonlinearity=<span class="string">&#x27;leaky_relu&#x27;</span></span>):</span></span><br><span class="line"></span><br><span class="line">    fan = _calculate_correct_fan(tensor, mode)</span><br><span class="line">    gain = calculate_gain(nonlinearity, a)</span><br><span class="line">    std = gain / math.sqrt(fan)</span><br><span class="line">    bound = math.sqrt(<span class="number">3.0</span>) * std  <span class="comment"># Calculate uniform bounds from standard deviation</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">return</span> tensor.uniform_(-bound, bound)</span><br></pre></td></tr></table></figure>

<h3 id="3-推导的先验知识"><a href="#3-推导的先验知识" class="headerlink" title="3. 推导的先验知识"></a>3. 推导的先验知识</h3><h4 id="3-1-用变量来看待问题"><a href="#3-1-用变量来看待问题" class="headerlink" title="3.1 用变量来看待问题"></a>3.1 用变量来看待问题</h4><p>![图片](Kaiming He初始化的学习/640)</p>
<p>参照上面的卷积图, 对输入的特征图进行的卷积. 具体要研究的是输出的一个点的方差(紫色点). 所以是通过黄色的输入(个)和绿色的卷积参数(个)去计算一个输出值(紫色输出)的方差. <strong>一个点</strong>对应于原论文里面的说法为<strong>a response</strong>. 感觉这个是理解权重初始化的重点. 基于独立同分布的强假设: 输入的每个值都是独立同分布的, 所以和独立同分布的参数进行卷积得到结果的分布也是相同的. 所以其他的3个输出点的方差也是一样的. 进一步说, 虽然输入是个不同的值. 但是我们可以这样认为: <strong>有一个满足某分布的随机变量, 然后随机抽样48次, 这48个值就可以组成了输入, 且独立同分布(也可称输入的每个像素点是独立同分布的)</strong>. 卷积的参数也可以这么认为. 那么我们可以用一个随机变量表示48个输入, 也可以用一个随机变量表示27个卷积参数, 亦可以用一个随机变量表示4个输出值.</p>
<h4 id="3-2-几个公式"><a href="#3-2-几个公式" class="headerlink" title="3.2 几个公式"></a>3.2 几个公式</h4><p>$$<br>var(X_1+\cdots+X_n)=var(X_1)+\cdots+var(X_n)<br>$$</p>
<p>$(1)$式表示独立随机变量之和的方差等于各变量的方差之和, 如果$X_1$和$X_2$还是同分布的,那么$var(X_1)=var(X_2)\Rightarrow var(X_1)+var(X_2)=2var(X_1)=2var(X_2)$. 将这个应用在卷积求和的那一步(卷积先相乘, 再求和).<br>$$<br>var(X)=E(X^2)-(E(X))^2<br>$$<br>$(2)$式是通过期望求方差的公式, 方差等于平方的期望减去期望的平方. 如果$E(X)=0$, 那么$var(X)=E(X^2)$.<br>$$<br>var(XY)=var(X)var(Y)+var(X)(E(Y))^2+var(Y)var(E(X))^2<br>$$<br>$(3)$式是独立变量乘积的一个公式(协方差为0). 如果$E(X)=E(Y)=0$, 那么$var(XY)=var(X)var(Y)$.</p>
<h3 id="4-kaiming初始化"><a href="#4-kaiming初始化" class="headerlink" title="4. kaiming初始化"></a>4. kaiming初始化</h3><p>kaiming初始化的推导过程只包含卷积和ReLU激活函数, 默认是vgg类似的网络, 没有残差, concat之类的结构, 也没有BN层.<br>$$<br>Y_l = W_lX_l + B_l<br>$$<br>此处,$Y_l$表示<strong>某个位置的输出值</strong>，$X_l$表示被卷积的输入,有$k×k×c$形状(对应于上图的黄色部分), $k$表示卷积核的大小,$c$表示输入的通道.令$n=k×k×c$,则$n$的大小表示一个输出值是由多少个输入值计算出来的(求方差的时候用到).$W$有$d×n$形状, $d$表示的输出通道的数量.下标$l$表示第几层.$X_l=f(Y_{l-1})$, $f$表示激活函数ReLU, 表示前一层的输出经过激活函数变成下一层的输入. $c_l=d_{l-1}$表示网络下一层的输入通道数等于上一层的输出通道数.(这里是对应原论文的翻译了)</p>
<h4 id="4-1-前向传播时每层的方差都是1"><a href="#4-1-前向传播时每层的方差都是1" class="headerlink" title="4.1 前向传播时每层的方差都是1"></a>4.1 前向传播时每层的方差都是1</h4><p>因为一个输出的$y$是由$n$个输入的$x$和其$n$个权重相乘再求和得到的(卷积的过程), 且假设权重数值之间是独立同分布的,$x$数值之间也是独立同分布的,且$x$和权重相互独立。那么根据(1)式得<br>$$<br>var(y_l)=n_lvar(w_l\cdot x_l)<br>$$<br>其中的$y,w,x$都表示随机变量，$l$表示第几层。举个例子：<br>$$<br>y=w_1×x_1+w_2×x_2+w_3×x_3+w_4×x_4+w_5×x_5+w_6×x_6<br>$$<br>其中，$w_几×x_几$看作一个整体，且1到6之间相互独立，就能得到<br>$$<br>var(y)=var(w_1×x_1)+var(w_2×x_2)+var(w_3×x_3)+var(w_4×x_4)+var(w_5×x_5)+var(w_6×x_6)<br>$$<br>又如果$w_几×x_几$之间又是同分布的, 那么他们的方差就是相同的, 就能得到$var(y)=6var(w×x)$. 进一步,因为$w_l,x_l$是相互独立的, 所以根据(3)式，可将(5)式推导为<br>$$<br>var(y_l)=n_l[var(w_l)var(x_l)+var(w_l)(E(x_l))^2+(E(w_l))^2var(x_l)]<br>$$<br>初始化的时候令权重的均值是0, 且假设更新的过程中权重的均值一直是0,则$E(w_l)=0$,但是$x_l$是上一层通过ReLU得到的,所以$E(x_l)\ne0$.<br>$$<br>var(y_l)=n_l[var(w_l)var(x_l)+var(w_l)(E(x_l))^2=n_lvar(w_l)(var(x_l)+(E(x_l))^2)<br>$$<br>通过(2)式可得$var(x_l)+(E(x_l))^2=E(x_l^2)$,则（9）式推导为<br>$$<br>var(y_l)=n_lvar(w_l)E(x_l^2)<br>$$<br>接下来求$E(x_l^2)$, 通过第$l-1$层的输出来求此期望, 我们有$x_l=f(y_{l-1})$, 其中$f$表示ReLU函数.<br>$$<br>E(x_l^2)=E(f^2(y_{l-1}))=\int_{-\infty}^{+\infty}p(y_{l-1})(y_{l-1})^2dy_{l-1}<br>$$<br>因为$y_{l-1}\in(-\infty,0)$的时候$f(y_{l-1})=0$，所以可以去掉小于0的区间，并且大于0的时候$f(y_{l-1})=y_{l-1}$，所以可得<br>$$<br>E(x_l^2)=E(f^2(y_{l-1}))=\int_{0}^{+\infty}p(y_{l-1})(y_{l-1})^2dy_{l-1}<br>$$<br>现因为$w_{l-1}$是假设在0周围对称分布且均值为0, 所以$y_{l-1}$也是在0附近分布是对称的, 并且均值为0(此处假设偏置为0,). 则$\int_{0}^{+\infty}p(y_{l-1})(y_{l-1})^2dy_{l-1}=\int_{-\infty}^{0}p(y_{l-1})(y_{l-1})^2dy_{l-1}$, 进一步可以得到<br>$$<br>E(x_l^2)=E(f^2(y_{l-1}))=\frac 12(\int_{0}^{+\infty}p(y_{l-1})(y_{l-1})^2dy_{l-1}+\int_{-\infty}^{0}p(y_{l-1})(y_{l-1})^2dy_{l-1})=\frac 12\int_{-\infty}^{+\infty}p(y_{l-1})(y_{l-1})^2dy_{l-1}=\frac 12E(y^2_{l-1})<br>$$<br>现在通过公式(2),$var(y_{l-1})=E(y^2_{l-1})-(E(y_{l-1}))^2$ ,其中$y_{l-1}$的均值是0, 则$var(y_{l-1})=E(y^2_{l-1})$,那么（13）式可进一步推导为<br>$$<br>E(x^2_l)=\frac 12E(y^2_{l-1})=\frac 12var(y_{l-1})<br>$$<br>将(14)式带入(10)式则为<br>$$<br>var(y_l)=\frac 12n_lvar(w_l)var(y_{l-1})<br>$$<br>然后从第一层一直往前进行前向传播, 可以得到某层的方差为<br>$$<br>var(y_l)=var(y_1)(\prod_{i=0}^L\frac 12n_lvar(w_l))<br>$$<br>这里的$y_1$就是输入的样本, 我们会将其归一化处理, 所以$var(y_1)=1$, 现在让每层输出方差等于1, 即<br>$$<br>\frac 12n_lvar(w_l)=1<br>$$</p>
<p>$$<br>var(w_l)=\frac 2{n_l}<br>$$</p>
<p>举例层卷积, 输入大小为$32×16×16$, 分别表示通道数量、高、宽, 卷积核大小为$64×32×3×3$, 分别表示输出通道数量、输入通道数量、卷积核高、卷积核宽. 则该层的权重$w\sim N(0,\frac 2{32×3×3})$, 偏置初始化为0. $64×32×3×3=18432$个参数都是从这个分布里面采样. 也对应了Pytorch里面的kaiming初始化只要传卷积核的参数进去就行了, 可以看下源码对应的计算.</p>
<h4 id="4-2-反向传播时梯度的方差都是1"><a href="#4-2-反向传播时梯度的方差都是1" class="headerlink" title="4.2 反向传播时梯度的方差都是1"></a>4.2 反向传播时梯度的方差都是1</h4><p>$$<br>\Delta X_l=\hat W_l\Delta Y_l<br>$$</p>
<p>其中, $\Delta$表示损失函数对其求导. 与正常的反向传播推导不一样, 这里假设$\Delta Y_l$表示$d$个通道,每个通道$k×k$大小,$\hat n=k×k×d$ ,与正向传播的时候一样, $\Delta X_l$有个$c$通道, $\Delta Y_l$有$d$个通道. $\hat W_l$的大小为$c×\hat n$,所以$\Delta X_l$的形状为$c×1$.$\hat W$和$W$只差了一个转置(涉及到反向传播). 同样的想法是, 一个$\Delta x_l$的值是很多个$\Delta y_l$求得到, 继续通过多个独立同分布变量求一个变量(梯度)的方差. 假设随机变量$\hat w_l,\Delta y_l$都是独立同分布的,$\hat w_l$的分布在0附近对称的, 则$\Delta x_l$对每层$l$,均值都是0, 即$E(\Delta x_l)=0$. 因为前向传播的时候<br>$$<br>x_{l+1}=f(y_l)<br>$$<br>所以反向传播则为<br>$$<br>\Delta y_l=f’(y_l)\Delta x_{l+1}<br>$$<br>又因为$f$是ReLU, 导数要么是0要么是1, 那么假设两者各占一半, 同时假设$f’(y_l)$和$\Delta x_{l+1}$相互独立.那么<br>$$<br>E(\Delta y_l)=\frac 12×0×\Delta x_{l+1}+\frac 12×1×\Delta x_{l+1}=\frac 12E(\Delta x_{l+1})=0<br>$$<br>其中, 将概率分为了两部分,一部分对应的ReLU导数为0，一部分对应的ReLU导数为1 (且这两部分假设都是50%的可能). 公式（22）表示对于一个$\Delta y_l$的取值, 有一半概率对应ReLU导数为0，一般对应为1. 根据(2)式又得<br>$$<br>var(\Delta y_l)=E(\Delta y^2_l)<br>$$</p>
<p>$$<br>var(\Delta y_l)=var(f’(y_l)\Delta x_{l+1})=\frac 12var(0\Delta x_{l+1})+\frac 12var(1\Delta x_{l+1})=\frac 12var(\Delta x_{l+1})<br>$$</p>
<p>(24)式也可以通过(13)式用类似的方法求出. 那么,<br>$$<br>var(\Delta x_l)=\hat nvar(\hat w_l\Delta y_l)=\hat n[var(\hat w_l)var(\Delta y_l)+var(\hat w_l)(E(\Delta y_l))^2+var(\Delta y_l)(E(\hat w_l))^2]=\hat nvar(\hat w_l)var(\Delta y_l)=\frac 12\hat nvar(\hat w_l)var(\Delta x_{l+1})<br>$$<br>所以,按照前向推导的方法,最后得出的公式是<br>$$<br>\frac 12\hat n_lvar(w_l)=1<br>$$<br>按照前向传播最后的示例, 此处的应该为$w\sim N(0,\frac 2{64×3×3})$</p>
</div><div class="post-copyright"><blockquote><p>原文作者: 贺同学</p><p>原文链接: <a href="http://clarkhedi.github.io/2021/04/17/kaiming-he-chu-shi-hua-de-xue-xi/">http://clarkhedi.github.io/2021/04/17/kaiming-he-chu-shi-hua-de-xue-xi/</a></p><p>版权声明: 转载请注明出处(必须保留原文作者署名原文链接)</p></blockquote></div><div class="tags"><a href="/tags/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/">论文学习</a></div><div class="post-share"><div class="social-share"><span>分享到:</span></div><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到:</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2021/05/08/pyqt5-xue-xi-bi-ji-yi/" class="pre">PyQt5学习笔记（一）</a><a href="/2021/04/05/autograd-yu-luo-ji-hui-gui/" class="next">autograd与逻辑回归</a></div><div id="comments"><div id="container"><script type="text/javascript" src="https://unpkg.com/gitalk/dist/gitalk.min.js?v=2.0.5"></script><script type="text/javascript" src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js?v=2.0.5"></script><script>var gitalk = new Gitalk({
  clientID: 'd6d4fb51a5284e4df5fd',
  clientSecret: '80ba8b4586bfce6df672e814df089b31e49a5549',
  repo: 'Gitalk',
  owner: 'clarkhedi',
  admin: ['clarkhedi'],
  id: md5(window.location.pathname),
  distractionFreeMode: false,
  language: 'zh-CN',
  pagerDirection: 'last'
})
gitalk.render('container')</script></div></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%A5%BD%E7%9A%84%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">1. 为什么需要好的权重初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-kaiming%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%96%B9%E6%B3%95"><span class="toc-text">2. kaiming初始化的两个方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E5%85%88%E6%9D%A5%E4%B8%AA%E7%BB%93%E8%AE%BA"><span class="toc-text">2.1 先来个结论</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E5%86%8D%E6%9D%A5%E4%B8%AA%E6%BA%90%E7%A0%81"><span class="toc-text">2.2 再来个源码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%8E%A8%E5%AF%BC%E7%9A%84%E5%85%88%E9%AA%8C%E7%9F%A5%E8%AF%86"><span class="toc-text">3. 推导的先验知识</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E7%94%A8%E5%8F%98%E9%87%8F%E6%9D%A5%E7%9C%8B%E5%BE%85%E9%97%AE%E9%A2%98"><span class="toc-text">3.1 用变量来看待问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E5%87%A0%E4%B8%AA%E5%85%AC%E5%BC%8F"><span class="toc-text">3.2 几个公式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-kaiming%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">4. kaiming初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%97%B6%E6%AF%8F%E5%B1%82%E7%9A%84%E6%96%B9%E5%B7%AE%E9%83%BD%E6%98%AF1"><span class="toc-text">4.1 前向传播时每层的方差都是1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%97%B6%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%96%B9%E5%B7%AE%E9%83%BD%E6%98%AF1"><span class="toc-text">4.2 反向传播时梯度的方差都是1</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/11/18/hexo-start/">Hexo Start</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/16/ge-xing-hua-ding-zhi-ni-de-github-zhu-ye/">2021年，教程 | 个性化定制你的 GitHub 主页</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/07/you-mei-de-sublime-text/">优美的Sublime Text</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/08/pyqt5-xue-xi-bi-ji-yi/">PyQt5学习笔记（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/17/kaiming-he-chu-shi-hua-de-xue-xi/">Kaiming He初始化的学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/autograd-yu-luo-ji-hui-gui/">autograd与逻辑回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/ji-suan-tu-yu-dong-tai-tu-ji-zhi/">计算图与动态图机制</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/zhang-liang-cao-zuo-yu-xian-xing-hui-gui/">张量操作与线性回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/tensor-zhang-liang-jie-shao/">Tensor（张量）介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/13/triplet-loss-xue-xi/">triplet-loss学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/GitHub%E5%AD%A6%E4%B9%A0/">GitHub学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo%E5%AD%A6%E4%B9%A0/">Hexo学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Jupyter%E8%AF%AD%E6%B3%95/">Jupyter语法</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux%E5%AD%A6%E4%B9%A0/">Linux学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/OpenCV-Python%E6%89%8B%E8%AE%B0/">OpenCV-Python手记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Python学习笔记</a><span class="category-list-count">8</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/PyQt5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">PyQt5学习笔记</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Pytorch学习笔记</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E7%B1%BB%E6%8A%80%E6%9C%AF/">分类技术</a><span class="category-list-count">14</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E7%B1%BB%E6%8A%80%E6%9C%AF/LBP%E5%AD%A6%E4%B9%A0/">LBP学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E7%B1%BB%E6%8A%80%E6%9C%AF/SVM%E5%85%AB%E8%82%A1/">SVM八股</a><span class="category-list-count">10</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">图像处理学习笔记</a><span class="category-list-count">7</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB/">车牌识别</a><span class="category-list-count">7</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AE%9E%E9%AA%8C%E6%A5%BC%E5%AD%A6%E4%B9%A0/">实验楼学习</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E8%B8%A9%E5%9D%91/">环境搭建踩坑</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/">英语学习</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">论文学习笔记</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/CLBP/" style="font-size: 15px;">CLBP</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">论文学习</a> <a href="/tags/vim/" style="font-size: 15px;">vim</a> <a href="/tags/OpenCV/" style="font-size: 15px;">OpenCV</a> <a href="/tags/PyQt5/" style="font-size: 15px;">PyQt5</a> <a href="/tags/GUI/" style="font-size: 15px;">GUI</a> <a href="/tags/%E8%AE%B0%E5%8D%95%E8%AF%8D/" style="font-size: 15px;">记单词</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/GitHub/" style="font-size: 15px;">GitHub</a> <a href="/tags/Resume/" style="font-size: 15px;">Resume</a> <a href="/tags/LBP/" style="font-size: 15px;">LBP</a> <a href="/tags/LPR/" style="font-size: 15px;">LPR</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/">2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/">2021</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">36</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">5</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> 友情链接</i></div><ul></ul><a href="https://github.com/chaooo/hexo-theme-BlueLake" title="BlueLake主题" target="_blank">BlueLake主题</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247484443&amp;idx=1&amp;sn=7110e42ef9e95a8c16064dde5b897960&amp;chksm=e870d556df075c4053a90d207c078e5fe965b5f9004de7f34dee09fccf1a37977aa3860b3a96&amp;mpshare=1&amp;scene=23&amp;srcid=0428XfRkvl0HsaHMU71k04dq#rd" title="重磅|完备的AI学习路线,最详细的资源整理！" target="_blank">重磅|完备的AI学习路线,最详细的资源整理！</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzI5NDY1MjQzNA==&amp;mid=2247489096&amp;idx=1&amp;sn=ca2e1d72c9decd021c0365cc9e93a539&amp;chksm=ec5ec935db2940237482c5581b7a45f2bd50b138efa6488ed9b942c182486a4e74131e9af523&amp;mpshare=1&amp;scene=23&amp;srcid=#rd" title="推荐|机器学习入门方法和资料合集" target="_blank">推荐|机器学习入门方法和资料合集</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzU4NTY4Mzg1Mw==&amp;mid=100000497&amp;idx=1&amp;sn=96cc795f1c22f7da2f3260d4ea364ff3&amp;chksm=7d8784134af00d05e41da36475bca417851535ed9710c7bd44ae7365cf40857015d922671c95&amp;mpshare=1&amp;scene=23&amp;srcid=04138MekLgFi3vNAz2E2rTrv#rd" title="良心推荐：机器学习入门资料汇总及学习建议（2018版）" target="_blank">良心推荐：机器学习入门资料汇总及学习建议（2018版）</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&amp;mid=2649032678&amp;idx=1&amp;sn=7ce30241f24dae790f7361173132ee04&amp;chksm=8712b99bb065308d6626ac71d358d386199f5fdcc0d20f48b9ff7f7f84f32892325075413d78&amp;mpshare=1&amp;scene=1&amp;srcid=1213VnRvWgKRhAswA6qj9C6U&amp;sharer_sharetime=1607823235596&amp;sharer_shareid=6f3d2b6e1888cc6674af1cb9d8f856b5&amp;key=94e16379e8fc498f20603adf1c7c1474460f32c861df423924a01860bf66b23e3b54ec715f59513ed57f7be44b91602324165303caa8015279359147bf99dc0ac5c9482bbf323df6415707db1df1dbb44795972834805221b6810c0783bc61213b424afc1f9c747d8eddfe17782e3165ddfff5144eff7867ea82ee34ab2e6e44&amp;ascene=1&amp;uin=MzMyMzAzNzE5OA%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=en&amp;exportkey=A%2Fn4vOWzACQbiXdS3ts1%2BmU%3D&amp;pass_ticket=X0sXcXVfthe9ne%2BkdsNidLJDaUos8pPBg0qSPgQJE8kY3CQmybx%2FB2TVL1w9Glu6&amp;wx_header=0" title="深度学习CV算法工程师从入门到初级面试" target="_blank">深度学习CV算法工程师从入门到初级面试</a></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">贺同学.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a target="_blank" rel="noopener" href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.5"></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="/js/imgview.js?v=2.0.5" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.5" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>
<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.5"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.5"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><link rel="stylesheet" type="text/css" href="https://unpkg.com/gitalk/dist/gitalk.css?v=2.0.5"><title>Tensor（张量）介绍 | 我中意你23332</title><meta name="generator" content="Hexo 5.2.0"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Tensor（张量）介绍</h1><a id="logo" href="/.">我中意你23332</a><p class="description">Tomorrow comes never.</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">Tensor（张量）介绍</h1><div class="post-meta"><a href="/2021/04/05/tensor-zhang-liang-jie-shao/#comments" class="comment-count"></a><p><span class="date">Apr 05, 2021</span><span><a href="/categories/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="category">Pytorch学习笔记</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><h2 id="Tensor-张量-介绍"><a href="#Tensor-张量-介绍" class="headerlink" title="Tensor(张量)介绍"></a>Tensor(张量)介绍</h2><h3 id="1、Tensor-的概念"><a href="#1、Tensor-的概念" class="headerlink" title="1、Tensor 的概念"></a>1、Tensor 的概念</h3><p>Tensor 中文为张量。张量的意思是一个多维数组，它是标量、向量、矩阵的高维扩展。</p>
<p>例如：标量可以称为 0 维张量，向量可以称为 1 维张量，矩阵可以称为 2 维张量，RGB 图像可以表示 3 维张量。你也可以把张量看作多维数组。</p>
<img src="/2021/04/05/tensor-zhang-liang-jie-shao/20200515144610.png" class="" title="img">

<h3 id="2、Tensor-与-Variable"><a href="#2、Tensor-与-Variable" class="headerlink" title="2、Tensor 与 Variable"></a>2、Tensor 与 Variable</h3><p>在 PyTorch 0.4.0 之前，torch.autograd 包中存在 Variable 这种数据类型，主要是用于封装 Tensor，进行自动求导。Variable 主要包含下面几种属性。</p>
<ul>
<li>data: 被包装的 Tensor。</li>
<li>grad: data 的梯度。</li>
<li>grad_fn: 创建 Tensor 所使用的 Function，是自动求导的关键，因为根据所记录的函数才能计算出导数。</li>
<li>requires_grad: 指示是否需要梯度，并不是所有的张量都需要计算梯度。</li>
<li>is_leaf: 指示是否叶子节点(张量)，叶子节点的概念在计算图中会用到，后面详细介绍。</li>
</ul>
<img src="/2021/04/05/tensor-zhang-liang-jie-shao/20200515145120.png" class="" title="img">

<p>在 PyTorch 0.4.0 之后，Variable 并入了 Tensor。在之后版本的 Tensor 中，除了具有上面 Variable 的 5 个属性，还有另外 3 个属性。</p>
<ul>
<li><p>dtype: 张量的数据类型，如 torch.FloatTensor，torch.cuda.FloatTensor。</p>
</li>
<li><p>shape: 张量的形状。如 (64, 3, 224, 224)</p>
</li>
<li><p>device: 张量所在设备 (CPU/GPU)，GPU 是加速计算的关键</p>
</li>
</ul>
<img src="/2021/04/05/tensor-zhang-liang-jie-shao/20200515145801.png" class="" title="img">

<p>关于 dtype，PyTorch 提供了 9 种数据类型，共分为 3 大类：<code>float (16-bit, 32-bit, 64-bit)</code>、<code>integer (unsigned-8-bit ,8-bit, 16-bit, 32-bit, 64-bit)</code>、<code>Boolean</code>。模型参数和数据用的最多的类型是 float-32-bit。label 常用的类型是 integer-64-bit。</p>
<img src="/2021/04/05/tensor-zhang-liang-jie-shao/20200515150439.png" class="" title="img">

<h3 id="3、Tensor-创建的方法"><a href="#3、Tensor-创建的方法" class="headerlink" title="3、Tensor 创建的方法"></a>3、Tensor 创建的方法</h3><h4 id="利用torch直接创建tensor"><a href="#利用torch直接创建tensor" class="headerlink" title="利用torch直接创建tensor"></a>利用torch直接创建tensor</h4><h5 id="torch-tensor"><a href="#torch-tensor" class="headerlink" title="torch.tensor()"></a><code>torch.tensor()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor(data, dtype=<span class="literal">None</span>, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>, pin_memory=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>data: 数据，可以是 list，numpy</li>
<li>dtype: 数据类型，默认与 data 的一致</li>
<li>device: 所在设备，cuda/cpu</li>
<li>requires_grad: 是否需要梯度</li>
<li>pin_memory: 是否存于锁页内存</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">arr = np.ones((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">print(<span class="string">&quot;ndarray的数据类型：&quot;</span>, arr.dtype)</span><br><span class="line"><span class="comment"># 创建存放在 GPU 的数据</span></span><br><span class="line"><span class="comment"># t = torch.tensor(arr, device=&#x27;cuda&#x27;)</span></span><br><span class="line">t = torch.tensor(arr)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ndarray的数据类型：float64</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">	     [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">	     [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure>

<h4 id="从-numpy-创建-tensor"><a href="#从-numpy-创建-tensor" class="headerlink" title="从 numpy 创建 tensor"></a>从 numpy 创建 tensor</h4><h5 id="torch-from-numpy-ndarray"><a href="#torch-from-numpy-ndarray" class="headerlink" title="torch.from_numpy(ndarray)"></a><code>torch.from_numpy(ndarray)</code></h5><p>利用这个方法创建的 tensor 和原来的 ndarray 共享内存，当修改其中一个数据，另外一个也会被改动。</p>
<img src="/2021/04/05/tensor-zhang-liang-jie-shao/20200515161227.png" class="" title="img">

<p> 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">arr = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">t = torch.from_numpy(arr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 array，tensor 也会被修改</span></span><br><span class="line"><span class="comment"># print(&quot;\n修改arr&quot;)</span></span><br><span class="line"><span class="comment"># arr[0, 0] = 0</span></span><br><span class="line"><span class="comment"># print(&quot;numpy array: &quot;, arr)</span></span><br><span class="line"><span class="comment"># print(&quot;tensor : &quot;, t)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 tensor，array 也会被修改</span></span><br><span class="line">print(<span class="string">&quot;\n修改tensor&quot;</span>)</span><br><span class="line">t[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">-1</span></span><br><span class="line">print(<span class="string">&quot;numpy array: &quot;</span>, arr)</span><br><span class="line">print(<span class="string">&quot;tensor : &quot;</span>, t)</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">修改tensor</span><br><span class="line">numpy array:  [[<span class="number">-1</span>  <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line"> [ <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>]]</span><br><span class="line">tensor :  tensor([[<span class="number">-1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>]], dtype=torch.int32)</span><br></pre></td></tr></table></figure>

<h4 id="根据数值创建-tensor"><a href="#根据数值创建-tensor" class="headerlink" title="根据数值创建 tensor"></a>根据数值创建 tensor</h4><h5 id="torch-zeros"><a href="#torch-zeros" class="headerlink" title="torch.zeros()"></a><code>torch.zeros()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros(*size, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：根据 size 创建全 0 张量</p>
<ul>
<li>size: 张量的形状</li>
<li>out: 输出的张量，如果指定了 out，那么<code>torch.zeros()</code>返回的张量和 out 指向的是同一个地址</li>
<li>layout: 内存中布局形式，有 strided，sparse_coo 等。当是稀疏矩阵时，设置为 sparse_coo 可以减少内存占用。</li>
<li>device: 所在设备，cuda/cpu</li>
<li>requires_grad: 是否需要梯度</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">out_t = torch.tensor([<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 这里制定了 out</span></span><br><span class="line">t = torch.zeros((<span class="number">3</span>, <span class="number">3</span>), out=out_t)</span><br><span class="line">print(t, <span class="string">&#x27;\n&#x27;</span>, out_t)</span><br><span class="line"><span class="comment"># id 是取内存地址。最终 t 和 out_t 是同一个内存地址</span></span><br><span class="line">print(<span class="built_in">id</span>(t), <span class="built_in">id</span>(out_t), <span class="built_in">id</span>(t) == <span class="built_in">id</span>(out_t))</span><br></pre></td></tr></table></figure>

<p>输出是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]) </span><br><span class="line"> tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="number">2984903203072</span> <span class="number">2984903203072</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<h5 id="torch-zeros-like"><a href="#torch-zeros-like" class="headerlink" title="torch.zeros_like"></a><code>torch.zeros_like</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros_like(<span class="built_in">input</span>, dtype=<span class="literal">None</span>, layout=<span class="literal">None</span>, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>, memory_format=torch.preserve_format)</span><br></pre></td></tr></table></figure>

<p>功能：根据 input 形状创建全 0 张量</p>
<ul>
<li>input: 创建与 input 同形状的全 0 张量</li>
<li>dtype: 数据类型</li>
<li>layout: 内存中布局形式，有 strided，sparse_coo 等。当是稀疏矩阵时，设置为 sparse_coo 可以减少内存占用。</li>
</ul>
<p>同理还有全 1 张量的创建方法：<code>torch.ones()</code>，<code>torch.ones_like()</code>。</p>
<h5 id="torch-full-，torch-full-like"><a href="#torch-full-，torch-full-like" class="headerlink" title="torch.full()，torch.full_like()"></a><code>torch.full()</code>，<code>torch.full_like()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.full(size, fill_value, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：创建自定义数值的张量</p>
<ul>
<li>size: 张量的形状，如 (3,3)</li>
<li>fill_value: 张量中每一个元素的值</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t = torch.full((<span class="number">3</span>, <span class="number">3</span>), <span class="number">1</span>)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>

<h5 id="torch-arange"><a href="#torch-arange" class="headerlink" title="torch.arange()"></a><code>torch.arange()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.arange(start=<span class="number">0</span>, end, step=<span class="number">1</span>, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：创建等差的 1 维张量。注意区间为[start, end)。</p>
<ul>
<li>start: 数列起始值</li>
<li>end: 数列结束值，开区间，取不到结束值</li>
<li>step: 数列公差，默认为 1</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t = torch.arange(<span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure>

<h5 id="torch-linspace"><a href="#torch-linspace" class="headerlink" title="torch.linspace()"></a><code>torch.linspace()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.linspace(start, end, steps=<span class="number">100</span>, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：创建均分的 1 维张量。数值区间为 [start, end]</p>
<ul>
<li>start: 数列起始值</li>
<li>end: 数列结束值</li>
<li>steps: 数列长度 (元素个数)</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># t = torch.linspace(2, 10, 5)</span></span><br><span class="line">t = torch.linspace(<span class="number">2</span>, <span class="number">10</span>, <span class="number">6</span>)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ <span class="number">2.0000</span>,  <span class="number">3.6000</span>,  <span class="number">5.2000</span>,  <span class="number">6.8000</span>,  <span class="number">8.4000</span>, <span class="number">10.0000</span>])</span><br></pre></td></tr></table></figure>

<h5 id="torch-logspace"><a href="#torch-logspace" class="headerlink" title="torch.logspace()"></a><code>torch.logspace()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.logspace(start, end, steps=<span class="number">100</span>, base=<span class="number">10.0</span>, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：创建对数均分的 1 维张量。数值区间为 [start, end]，底为 base。</p>
<ul>
<li>start: 数列起始值</li>
<li>end: 数列结束值</li>
<li>steps: 数列长度 (元素个数)</li>
<li>base: 对数函数的底，默认为 10</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># t = torch.linspace(2, 10, 5)</span></span><br><span class="line">t = torch.linspace(<span class="number">2</span>, <span class="number">10</span>, <span class="number">6</span>)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ <span class="number">2.0000</span>,  <span class="number">3.6000</span>,  <span class="number">5.2000</span>,  <span class="number">6.8000</span>,  <span class="number">8.4000</span>, <span class="number">10.0000</span>])</span><br></pre></td></tr></table></figure>

<h5 id="torch-eye"><a href="#torch-eye" class="headerlink" title="torch.eye()"></a><code>torch.eye()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eye(n, m=<span class="literal">None</span>, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：创建单位对角矩阵( 2 维张量)，默认为方阵</p>
<ul>
<li>n: 矩阵行数。通常只设置 n，为方阵。</li>
<li>m: 矩阵列数</li>
</ul>
<h4 id="根据概率创建-Tensor"><a href="#根据概率创建-Tensor" class="headerlink" title="根据概率创建 Tensor"></a>根据概率创建 Tensor</h4><h5 id="torch-normal"><a href="#torch-normal" class="headerlink" title="torch.normal()"></a><code>torch.normal()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.normal(mean, std, *, generator=<span class="literal">None</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>功能：生成正态分布 (高斯分布)</p>
<ul>
<li>mean: 均值</li>
<li>std: 标准差</li>
</ul>
<p>有 4 种模式：</p>
<ol>
<li><p>mean 为标量，std 为标量。这时需要设置 size。</p>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mean：标量 std: 标量</span></span><br><span class="line"><span class="comment"># 这里需要设置 size</span></span><br><span class="line">t_normal = torch.normal(<span class="number">0.</span>, <span class="number">1.</span>, size=(<span class="number">4</span>,))</span><br><span class="line">print(t_normal)</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0.6614, 0.2669, 0.0617, 0.6213])</span><br></pre></td></tr></table></figure>
</li>
<li><p>mean 为标量，std 为张量</p>
</li>
<li><p>mean 为张量，std 为标量</p>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mean：张量 std: 标量</span></span><br><span class="line">mean = torch.arange(<span class="number">1</span>, <span class="number">5</span>, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">std = <span class="number">1</span></span><br><span class="line">t_normal = torch.normal(mean, std)</span><br><span class="line">print(<span class="string">&quot;mean:&#123;&#125;\nstd:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(mean, std))</span><br><span class="line">print(t_normal)</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mean:tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line">std:<span class="number">1</span></span><br><span class="line">tensor([<span class="number">1.6614</span>, <span class="number">2.2669</span>, <span class="number">3.0617</span>, <span class="number">4.6213</span>])</span><br></pre></td></tr></table></figure>

<p>这 4 个数采样分布的均值不同，但是方差都是 1。</p>
</li>
<li><p>mean 为张量，std 为张量</p>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mean：张量 std: 张量</span></span><br><span class="line">mean = torch.arange(<span class="number">1</span>, <span class="number">5</span>, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">std = torch.arange(<span class="number">1</span>, <span class="number">5</span>, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">t_normal = torch.normal(mean, std)</span><br><span class="line">print(<span class="string">&quot;mean:&#123;&#125;\nstd:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(mean, std))</span><br><span class="line">print(t_normal)</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mean:tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line">std:tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line">tensor([<span class="number">1.6614</span>, <span class="number">2.5338</span>, <span class="number">3.1850</span>, <span class="number">6.4853</span>])</span><br></pre></td></tr></table></figure>

<p>其中 1.6614 是从正态分布 $N(1,1)$ 中采样得到的，其他数字以此类推。</p>
</li>
</ol>
<h5 id="torch-randn-和-torch-randn-like"><a href="#torch-randn-和-torch-randn-like" class="headerlink" title="torch.randn() 和 torch.randn_like()"></a><code>torch.randn() 和 torch.randn_like()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(*size, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：生成标准正态分布。</p>
<ul>
<li>size: 张量的形状</li>
</ul>
<h5 id="torch-rand-和-torch-rand-like"><a href="#torch-rand-和-torch-rand-like" class="headerlink" title="torch.rand() 和 torch.rand_like()"></a><code>torch.rand() 和 torch.rand_like()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.rand(*size, out=<span class="literal">None</span>, dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：在区间 [0, 1) 上生成均匀分布。</p>
<h5 id="torch-randint-和-torch-randint-like"><a href="#torch-randint-和-torch-randint-like" class="headerlink" title="torch.randint() 和 torch.randint_like()"></a><code>torch.randint() 和 torch.randint_like()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">randint(low=<span class="number">0</span>, high, size, *, generator=<span class="literal">None</span>, out=<span class="literal">None</span>,</span><br><span class="line">dtype=<span class="literal">None</span>, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：在区间 [low, high) 上生成整数均匀分布。</p>
<ul>
<li>size: 张量的形状</li>
</ul>
<h5 id="torch-randperm"><a href="#torch-randperm" class="headerlink" title="torch.randperm()"></a><code>torch.randperm()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randperm(n, out=<span class="literal">None</span>, dtype=torch.int64, layout=torch.strided, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>功能：生成从 0 到 n-1 的随机排列。常用于生成索引。</p>
<ul>
<li>n: 张量的长度</li>
</ul>
<h5 id="torch-bernoulli"><a href="#torch-bernoulli" class="headerlink" title="torch.bernoulli()"></a><code>torch.bernoulli()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.bernoulli(<span class="built_in">input</span>, *, generator=<span class="literal">None</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>功能：以 input 为概率，生成伯努利分布 (0-1 分布，两点分布)</p>
<ul>
<li>input: 概率值</li>
</ul>
</div><div class="post-copyright"><blockquote><p>原文作者: 贺同学</p><p>原文链接: <a href="http://clarkhedi.github.io/2021/04/05/tensor-zhang-liang-jie-shao/">http://clarkhedi.github.io/2021/04/05/tensor-zhang-liang-jie-shao/</a></p><p>版权声明: 转载请注明出处(必须保留原文作者署名原文链接)</p></blockquote></div><div class="tags"><a href="/tags/Pytorch/">Pytorch</a></div><div class="post-share"><div class="social-share"><span>分享到:</span></div><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到:</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2021/04/05/zhang-liang-cao-zuo-yu-xian-xing-hui-gui/" class="pre">张量操作与线性回归</a><a href="/2020/12/13/triplet-loss-xue-xi/" class="next">triplet-loss学习</a></div><div id="comments"><div id="container"><script type="text/javascript" src="https://unpkg.com/gitalk/dist/gitalk.min.js?v=2.0.5"></script><script type="text/javascript" src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js?v=2.0.5"></script><script>var gitalk = new Gitalk({
  clientID: 'd6d4fb51a5284e4df5fd',
  clientSecret: '80ba8b4586bfce6df672e814df089b31e49a5549',
  repo: 'Gitalk',
  owner: 'clarkhedi',
  admin: ['clarkhedi'],
  id: md5(window.location.pathname),
  distractionFreeMode: false,
  language: 'zh-CN',
  pagerDirection: 'last'
})
gitalk.render('container')</script></div></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor-%E5%BC%A0%E9%87%8F-%E4%BB%8B%E7%BB%8D"><span class="toc-text">Tensor(张量)介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81Tensor-%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-text">1、Tensor 的概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81Tensor-%E4%B8%8E-Variable"><span class="toc-text">2、Tensor 与 Variable</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81Tensor-%E5%88%9B%E5%BB%BA%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-text">3、Tensor 创建的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A9%E7%94%A8torch%E7%9B%B4%E6%8E%A5%E5%88%9B%E5%BB%BAtensor"><span class="toc-text">利用torch直接创建tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-tensor"><span class="toc-text">torch.tensor()</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8E-numpy-%E5%88%9B%E5%BB%BA-tensor"><span class="toc-text">从 numpy 创建 tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-from-numpy-ndarray"><span class="toc-text">torch.from_numpy(ndarray)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B9%E6%8D%AE%E6%95%B0%E5%80%BC%E5%88%9B%E5%BB%BA-tensor"><span class="toc-text">根据数值创建 tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-zeros"><span class="toc-text">torch.zeros()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-zeros-like"><span class="toc-text">torch.zeros_like</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-full-%EF%BC%8Ctorch-full-like"><span class="toc-text">torch.full()，torch.full_like()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-arange"><span class="toc-text">torch.arange()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-linspace"><span class="toc-text">torch.linspace()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-logspace"><span class="toc-text">torch.logspace()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-eye"><span class="toc-text">torch.eye()</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B9%E6%8D%AE%E6%A6%82%E7%8E%87%E5%88%9B%E5%BB%BA-Tensor"><span class="toc-text">根据概率创建 Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-normal"><span class="toc-text">torch.normal()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-randn-%E5%92%8C-torch-randn-like"><span class="toc-text">torch.randn() 和 torch.randn_like()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-rand-%E5%92%8C-torch-rand-like"><span class="toc-text">torch.rand() 和 torch.rand_like()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-randint-%E5%92%8C-torch-randint-like"><span class="toc-text">torch.randint() 和 torch.randint_like()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-randperm"><span class="toc-text">torch.randperm()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-bernoulli"><span class="toc-text">torch.bernoulli()</span></a></li></ol></li></ol></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/11/18/hexo-start/">Hexo Start</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/16/ge-xing-hua-ding-zhi-ni-de-github-zhu-ye/">2021年，教程 | 个性化定制你的 GitHub 主页</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/07/you-mei-de-sublime-text/">优美的Sublime Text</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/08/pyqt5-xue-xi-bi-ji-yi/">PyQt5学习笔记（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/17/kaiming-he-chu-shi-hua-de-xue-xi/">Kaiming He初始化的学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/autograd-yu-luo-ji-hui-gui/">autograd与逻辑回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/ji-suan-tu-yu-dong-tai-tu-ji-zhi/">计算图与动态图机制</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/zhang-liang-cao-zuo-yu-xian-xing-hui-gui/">张量操作与线性回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/tensor-zhang-liang-jie-shao/">Tensor（张量）介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/13/triplet-loss-xue-xi/">triplet-loss学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/GitHub%E5%AD%A6%E4%B9%A0/">GitHub学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo%E5%AD%A6%E4%B9%A0/">Hexo学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Jupyter%E8%AF%AD%E6%B3%95/">Jupyter语法</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux%E5%AD%A6%E4%B9%A0/">Linux学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/OpenCV-Python%E6%89%8B%E8%AE%B0/">OpenCV-Python手记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Python学习笔记</a><span class="category-list-count">8</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/PyQt5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">PyQt5学习笔记</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Pytorch学习笔记</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E7%B1%BB%E6%8A%80%E6%9C%AF/">分类技术</a><span class="category-list-count">14</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E7%B1%BB%E6%8A%80%E6%9C%AF/LBP%E5%AD%A6%E4%B9%A0/">LBP学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E7%B1%BB%E6%8A%80%E6%9C%AF/SVM%E5%85%AB%E8%82%A1/">SVM八股</a><span class="category-list-count">10</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">图像处理学习笔记</a><span class="category-list-count">7</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB/">车牌识别</a><span class="category-list-count">7</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AE%9E%E9%AA%8C%E6%A5%BC%E5%AD%A6%E4%B9%A0/">实验楼学习</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E8%B8%A9%E5%9D%91/">环境搭建踩坑</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/">英语学习</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">论文学习笔记</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/CLBP/" style="font-size: 15px;">CLBP</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">论文学习</a> <a href="/tags/vim/" style="font-size: 15px;">vim</a> <a href="/tags/OpenCV/" style="font-size: 15px;">OpenCV</a> <a href="/tags/PyQt5/" style="font-size: 15px;">PyQt5</a> <a href="/tags/GUI/" style="font-size: 15px;">GUI</a> <a href="/tags/%E8%AE%B0%E5%8D%95%E8%AF%8D/" style="font-size: 15px;">记单词</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/GitHub/" style="font-size: 15px;">GitHub</a> <a href="/tags/Resume/" style="font-size: 15px;">Resume</a> <a href="/tags/LBP/" style="font-size: 15px;">LBP</a> <a href="/tags/LPR/" style="font-size: 15px;">LPR</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/">2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/">2021</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">36</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">5</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> 友情链接</i></div><ul></ul><a href="https://github.com/chaooo/hexo-theme-BlueLake" title="BlueLake主题" target="_blank">BlueLake主题</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247484443&amp;idx=1&amp;sn=7110e42ef9e95a8c16064dde5b897960&amp;chksm=e870d556df075c4053a90d207c078e5fe965b5f9004de7f34dee09fccf1a37977aa3860b3a96&amp;mpshare=1&amp;scene=23&amp;srcid=0428XfRkvl0HsaHMU71k04dq#rd" title="重磅|完备的AI学习路线,最详细的资源整理！" target="_blank">重磅|完备的AI学习路线,最详细的资源整理！</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzI5NDY1MjQzNA==&amp;mid=2247489096&amp;idx=1&amp;sn=ca2e1d72c9decd021c0365cc9e93a539&amp;chksm=ec5ec935db2940237482c5581b7a45f2bd50b138efa6488ed9b942c182486a4e74131e9af523&amp;mpshare=1&amp;scene=23&amp;srcid=#rd" title="推荐|机器学习入门方法和资料合集" target="_blank">推荐|机器学习入门方法和资料合集</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzU4NTY4Mzg1Mw==&amp;mid=100000497&amp;idx=1&amp;sn=96cc795f1c22f7da2f3260d4ea364ff3&amp;chksm=7d8784134af00d05e41da36475bca417851535ed9710c7bd44ae7365cf40857015d922671c95&amp;mpshare=1&amp;scene=23&amp;srcid=04138MekLgFi3vNAz2E2rTrv#rd" title="良心推荐：机器学习入门资料汇总及学习建议（2018版）" target="_blank">良心推荐：机器学习入门资料汇总及学习建议（2018版）</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&amp;mid=2649032678&amp;idx=1&amp;sn=7ce30241f24dae790f7361173132ee04&amp;chksm=8712b99bb065308d6626ac71d358d386199f5fdcc0d20f48b9ff7f7f84f32892325075413d78&amp;mpshare=1&amp;scene=1&amp;srcid=1213VnRvWgKRhAswA6qj9C6U&amp;sharer_sharetime=1607823235596&amp;sharer_shareid=6f3d2b6e1888cc6674af1cb9d8f856b5&amp;key=94e16379e8fc498f20603adf1c7c1474460f32c861df423924a01860bf66b23e3b54ec715f59513ed57f7be44b91602324165303caa8015279359147bf99dc0ac5c9482bbf323df6415707db1df1dbb44795972834805221b6810c0783bc61213b424afc1f9c747d8eddfe17782e3165ddfff5144eff7867ea82ee34ab2e6e44&amp;ascene=1&amp;uin=MzMyMzAzNzE5OA%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=en&amp;exportkey=A%2Fn4vOWzACQbiXdS3ts1%2BmU%3D&amp;pass_ticket=X0sXcXVfthe9ne%2BkdsNidLJDaUos8pPBg0qSPgQJE8kY3CQmybx%2FB2TVL1w9Glu6&amp;wx_header=0" title="深度学习CV算法工程师从入门到初级面试" target="_blank">深度学习CV算法工程师从入门到初级面试</a></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">贺同学.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a target="_blank" rel="noopener" href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.5"></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="/js/imgview.js?v=2.0.5" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.5" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>
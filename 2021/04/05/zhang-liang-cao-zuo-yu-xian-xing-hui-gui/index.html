<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.5"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.5"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><link rel="stylesheet" type="text/css" href="https://unpkg.com/gitalk/dist/gitalk.css?v=2.0.5"><title>张量操作与线性回归 | 我中意你23332</title><meta name="generator" content="Hexo 5.2.0"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">张量操作与线性回归</h1><a id="logo" href="/.">我中意你23332</a><p class="description">Tomorrow comes never.</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">张量操作与线性回归</h1><div class="post-meta"><a href="/2021/04/05/zhang-liang-cao-zuo-yu-xian-xing-hui-gui/#comments" class="comment-count"></a><p><span class="date">Apr 05, 2021</span><span><a href="/categories/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="category">Pytorch学习笔记</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><h2 id="张量操作与线性回归"><a href="#张量操作与线性回归" class="headerlink" title="张量操作与线性回归"></a>张量操作与线性回归</h2><h3 id="1、张量的操作"><a href="#1、张量的操作" class="headerlink" title="1、张量的操作"></a>1、张量的操作</h3><h4 id="拼接"><a href="#拼接" class="headerlink" title="拼接"></a>拼接</h4><h5 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat()"></a><code>torch.cat()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat(tensors, dim=<span class="number">0</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>功能：将张量按照 dim 维度进行拼接</p>
<ul>
<li>tensors: 张量序列</li>
<li>dim: 要拼接的维度</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = torch.ones((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">t_0 = torch.cat([t, t], dim=<span class="number">0</span>)</span><br><span class="line">t_1 = torch.cat([t, t], dim=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">&quot;t_0:&#123;&#125; shape:&#123;&#125;\nt_1:&#123;&#125; shape:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(t_0, t_0.shape, t_1, t_1.shape))</span><br></pre></td></tr></table></figure>

<p>输出是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t_0:tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]) shape:torch.Size([<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">t_1:tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]) shape:torch.Size([<span class="number">2</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure>

<h5 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack()"></a><code>torch.stack()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack(tensors, dim=<span class="number">0</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>功能：将张量在新创建的 dim 维度上进行拼接</p>
<ul>
<li>tensors: 张量序列</li>
<li>dim: 要拼接的维度</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">t = torch.ones((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment"># dim =2</span></span><br><span class="line">t_stack = torch.stack([t, t, t], dim=<span class="number">2</span>)</span><br><span class="line">print(<span class="string">&quot;\nt_stack.shape:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(t_stack.shape))</span><br><span class="line"><span class="comment"># dim =0</span></span><br><span class="line">t_stack = torch.stack([t, t, t], dim=<span class="number">0</span>)</span><br><span class="line">print(<span class="string">&quot;\nt_stack.shape:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(t_stack.shape))</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t_stack.shape:torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">t_stack.shape:torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<p>注：第一次指定拼接的维度 dim =2，结果的维度是 [2, 3, 3]。后面指定拼接的维度 dim =0，由于原来的 tensor 已经有了维度 0，因此会把tensor 往后移动一个维度变为 [1,2,3]，再拼接变为 [3,2,3]。</p>
<h4 id="切分"><a href="#切分" class="headerlink" title="切分"></a>切分</h4><h5 id="torch-chunk"><a href="#torch-chunk" class="headerlink" title="torch.chunk()"></a><code>torch.chunk()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.chunk(<span class="built_in">input</span>, chunks, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>功能：将张量按照维度 dim 进行平均切分。若不能整除，则最后一份张量小于其他张量。</p>
<ul>
<li>input: 要切分的张量</li>
<li>chunks: 要切分的份数</li>
<li>dim: 要切分的维度</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones((<span class="number">2</span>, <span class="number">7</span>))  <span class="comment"># 7</span></span><br><span class="line">list_of_tensors = torch.chunk(a, dim=<span class="number">1</span>, chunks=<span class="number">3</span>)   <span class="comment"># 3</span></span><br><span class="line"><span class="keyword">for</span> idx, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(list_of_tensors):</span><br><span class="line">print(<span class="string">&quot;第&#123;&#125;个张量：&#123;&#125;, shape is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(idx+<span class="number">1</span>, t, t.shape))</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">第<span class="number">1</span>个张量：tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]), shape <span class="keyword">is</span> torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">第<span class="number">2</span>个张量：tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]), shape <span class="keyword">is</span> torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">第<span class="number">3</span>个张量：tensor([[<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>]]), shape <span class="keyword">is</span> torch.Size([<span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>注：由于 7 不能整除 3，7/3 再向上取整是 3，因此前两个维度是 [2, 3]，所以最后一个切分的张量维度是 [2,1]。</p>
<h5 id="torch-split"><a href="#torch-split" class="headerlink" title="torch.split()"></a><code>torch.split()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.split(tensor, split_size_or_sections, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>功能：将张量按照维度 dim 进行平均切分。可以指定每一个分量的切分长度。</p>
<ul>
<li>tensor: 要切分的张量</li>
<li>split_size_or_sections: 为 int 时，表示每一份的长度，如果不能被整除，则最后一份张量小于其他张量；为 list 时，按照 list 元素作为每一个分量的长度切分。如果 list 元素之和不等于切分维度 (dim) 的值，就会报错。</li>
<li>dim: 要切分的维度</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = torch.ones((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">list_of_tensors = torch.split(t, [<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>], dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> idx, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(list_of_tensors):</span><br><span class="line">print(<span class="string">&quot;第&#123;&#125;个张量：&#123;&#125;, shape is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(idx+<span class="number">1</span>, t, t.shape))</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">第<span class="number">1</span>个张量：tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]]), shape <span class="keyword">is</span> torch.Size([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">第<span class="number">2</span>个张量：tensor([[<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>]]), shape <span class="keyword">is</span> torch.Size([<span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">第<span class="number">3</span>个张量：tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]]), shape <span class="keyword">is</span> torch.Size([<span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>

<h4 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h4><h5 id="torch-index-select"><a href="#torch-index-select" class="headerlink" title="torch.index_select()"></a><code>torch.index_select()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.index_select(<span class="built_in">input</span>, dim, index, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>功能：在维度 dim 上，按照 index 索引取出数据拼接为张量 返回。</p>
<ul>
<li>input: 要索引的张量</li>
<li>dim: 要索引的维度</li>
<li>index: 要索引数据的序号</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建均匀分布</span></span><br><span class="line">t = torch.randint(<span class="number">0</span>, <span class="number">9</span>, size=(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment"># 注意 idx 的 dtype 不能指定为 torch.float</span></span><br><span class="line">idx = torch.tensor([<span class="number">0</span>, <span class="number">2</span>], dtype=torch.long)</span><br><span class="line"><span class="comment"># 取出第 0 行和第 2 行</span></span><br><span class="line">t_select = torch.index_select(t, dim=<span class="number">0</span>, index=idx)</span><br><span class="line">print(<span class="string">&quot;t:\n&#123;&#125;\nt_select:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(t, t_select))</span><br></pre></td></tr></table></figure>

<p>输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">t:</span><br><span class="line">tensor([[<span class="number">4</span>, <span class="number">2</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">3</span>, <span class="number">5</span>]])</span><br><span class="line">t_select:</span><br><span class="line">tensor([[<span class="number">4</span>, <span class="number">2</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">3</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure>

<h5 id="torch-mask-select"><a href="#torch-mask-select" class="headerlink" title="torch.mask_select()"></a><code>torch.mask_select()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.masked_select(<span class="built_in">input</span>, mask, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>功能：按照 mask 中的 True 进行索引拼接得到一维张量 返回。</p>
<ul>
<li>要索引的张量</li>
<li>mask: 与 input 同形状的布尔类型张量</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">t = torch.randint(<span class="number">0</span>, <span class="number">9</span>, size=(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">mask = t.le(<span class="number">5</span>)  <span class="comment"># ge is mean greater than or equal/   gt: greater than  le  lt</span></span><br><span class="line"><span class="comment"># 取出大于 5 的数</span></span><br><span class="line">t_select = torch.masked_select(t, mask)</span><br><span class="line">print(<span class="string">&quot;t:\n&#123;&#125;\nmask:\n&#123;&#125;\nt_select:\n&#123;&#125; &quot;</span>.<span class="built_in">format</span>(t, mask, t_select))</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">t:</span><br><span class="line">tensor([[<span class="number">5</span>, <span class="number">5</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">8</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line">mask:</span><br><span class="line">tensor([[ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">             [<span class="literal">False</span>, <span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">             [<span class="literal">False</span>,  <span class="literal">True</span>,  <span class="literal">True</span>]])</span><br><span class="line">t_select:</span><br><span class="line">tensor([<span class="number">5</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">7</span>]) </span><br></pre></td></tr></table></figure>

<p>注：最后返回的是一维张量。</p>
<h4 id="变换"><a href="#变换" class="headerlink" title="变换"></a>变换</h4><h5 id="torch-reshape"><a href="#torch-reshape" class="headerlink" title="torch.reshape()"></a><code>torch.reshape()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.reshape(<span class="built_in">input</span>, shape)</span><br></pre></td></tr></table></figure>

<p>功能：变换张量的形状。当张量在内存中是连续时，返回的张量和原来的张量共享数据内存，改变一个变量时，另一个变量也会被改变。</p>
<ul>
<li>input: 要变换的张量</li>
<li>shape: 新张量的形状</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成 0 到 8 的随机排列</span></span><br><span class="line">t = torch.randperm(<span class="number">8</span>)</span><br><span class="line"><span class="comment"># -1 表示这个维度是根据其他维度计算得出的(自动计算分配) [2x2x2=8]</span></span><br><span class="line">t_reshape = torch.reshape(t, (<span class="number">-1</span>, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">print(<span class="string">&quot;t:&#123;&#125;\nt_reshape:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(t, t_reshape))</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">t:tensor([<span class="number">1</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">4</span>])</span><br><span class="line">t_reshape:</span><br><span class="line">tensor([[[<span class="number">1</span>, <span class="number">7</span>],</span><br><span class="line">              [<span class="number">2</span>, <span class="number">5</span>]],</span><br><span class="line"></span><br><span class="line">            [[<span class="number">3</span>, <span class="number">6</span>],</span><br><span class="line">             [<span class="number">0</span>, <span class="number">4</span>]]])</span><br></pre></td></tr></table></figure>

<p>注：在上面代码的基础上，修改原来的张量的一个元素，新张量也会被改变。</p>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改张量 t 的第 0 个元素，张量 t_reshape 也会被改变</span></span><br><span class="line">t[<span class="number">0</span>] = <span class="number">1024</span></span><br><span class="line">print(<span class="string">&quot;t:&#123;&#125;\nt_reshape:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(t, t_reshape))</span><br><span class="line">print(<span class="string">&quot;t.data 内存地址:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">id</span>(t.data)))</span><br><span class="line">print(<span class="string">&quot;t_reshape.data 内存地址:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">id</span>(t_reshape.data)))</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">t:tensor([<span class="number">1024</span>,    <span class="number">4</span>,    <span class="number">3</span>,    <span class="number">6</span>,    <span class="number">5</span>,    <span class="number">2</span>,    <span class="number">7</span>,    <span class="number">0</span>])</span><br><span class="line">t_reshape:</span><br><span class="line">tensor([[[<span class="number">1024</span>,    <span class="number">4</span>],</span><br><span class="line">              [   <span class="number">3</span>,    <span class="number">6</span>]],</span><br><span class="line"></span><br><span class="line">            [[   <span class="number">5</span>,    <span class="number">2</span>],</span><br><span class="line">             [   <span class="number">7</span>,    <span class="number">0</span>]]])</span><br><span class="line">t.data 内存地址:<span class="number">2395125092288</span></span><br><span class="line">t_reshape.data 内存地址:<span class="number">2395125089152</span></span><br></pre></td></tr></table></figure>

<h5 id="torch-transpose"><a href="#torch-transpose" class="headerlink" title="torch.transpose()"></a><code>torch.transpose()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.transpose(<span class="built_in">input</span>, dim0, dim1)</span><br></pre></td></tr></table></figure>

<p>功能：交换张量的两个维度。常用于图像的变换，比如把<code>c*h*w</code>变换为<code>h*w*c</code>。</p>
<ul>
<li>input: 要交换的张量</li>
<li>dim0: 要交换的第一个维度</li>
<li>dim1: 要交换的第二个维度</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#把 c * h * w 变换为 h * w * c</span></span><br><span class="line">t = torch.rand((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="comment"># print(t)</span></span><br><span class="line">t_transpose_0 = torch.transpose(t, dim0=<span class="number">0</span>, dim1=<span class="number">1</span>)    <span class="comment"># c*h*w   -&gt;  h*c*w</span></span><br><span class="line"><span class="comment"># print(t_transpose_0)</span></span><br><span class="line">t_transpose_1 = torch.transpose(t_transpose_0, dim0=<span class="number">1</span>, dim1=<span class="number">2</span>)   <span class="comment"># c*h*w   -&gt;  h*w*c</span></span><br><span class="line"><span class="comment"># print(t_transpose_1)</span></span><br><span class="line">print(<span class="string">&quot;t shape:&#123;&#125;\nt_transpose shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(t.shape, t_transpose_1.shape))</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t shape:torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">t_transpose shape: torch.Size([<span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>

<h5 id="torch-t"><a href="#torch-t" class="headerlink" title="torch.t()"></a><code>torch.t()</code></h5><p>功能：2 维张量转置，对于 2 维矩阵而言，等价于<code>torch.transpose(input, 0, 1)</code>。</p>
<h5 id="torch-squeeze"><a href="#torch-squeeze" class="headerlink" title="torch.squeeze()"></a><code>torch.squeeze()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.squeeze(<span class="built_in">input</span>, dim=<span class="literal">None</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>功能：压缩长度为 1 的维度。</p>
<ul>
<li>dim: 若为 None，则移除所有长度为 1 的维度；若指定维度，则当且仅当该维度长度为 1 时可以移除。</li>
</ul>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 维度 0 和 3 的长度是 1</span></span><br><span class="line">t = torch.rand((<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 可以移除维度 0 和 3</span></span><br><span class="line">t_sq = torch.squeeze(t)</span><br><span class="line"><span class="comment"># 可以移除维度 0</span></span><br><span class="line">t_0 = torch.squeeze(t, dim=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 不能移除 1</span></span><br><span class="line">t_1 = torch.squeeze(t, dim=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">&quot;t.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(t.shape))</span><br><span class="line">print(<span class="string">&quot;t_sq.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(t_sq.shape))</span><br><span class="line">print(<span class="string">&quot;t_0.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(t_0.shape))</span><br><span class="line">print(<span class="string">&quot;t_1.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(t_1.shape))</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t.shape: torch.Size([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">t_sq.shape: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">t_0.shape: torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">t_1.shape: torch.Size([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<h5 id="torch-unsqueeze"><a href="#torch-unsqueeze" class="headerlink" title="torch.unsqueeze()"></a><code>torch.unsqueeze()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(<span class="built_in">input</span>, dim)</span><br></pre></td></tr></table></figure>

<p>功能：根据 dim 扩展维度，长度为 1。</p>
<h3 id="2、张量的数学运算"><a href="#2、张量的数学运算" class="headerlink" title="2、张量的数学运算"></a>2、张量的数学运算</h3><p>主要分为：加减乘除，对数，指数，幂函数 和三角函数。</p>
<p>这里介绍一下常用的几种方法。</p>
<h5 id="torch-add"><a href="#torch-add" class="headerlink" title="torch.add()"></a><code>torch.add()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.add(<span class="built_in">input</span>, other, out=<span class="literal">None</span>)</span><br><span class="line">torch.add(<span class="built_in">input</span>, other, *, alpha=<span class="number">1</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>功能：逐元素计算 input + alpha * other。因为在深度学习中经常用到先乘后加的操作。</p>
<ul>
<li>input: 第一个张量</li>
<li>alpha: 乘项因子</li>
<li>other: 第二个张量</li>
</ul>
<h5 id="torch-addcdiv"><a href="#torch-addcdiv" class="headerlink" title="torch.addcdiv()"></a><code>torch.addcdiv()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.addcdiv(<span class="built_in">input</span>, tensor1, tensor2, *, value=<span class="number">1</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>计算公式为：out $<em>{i}=\operatorname{input}</em>{i}+$ value $\times \frac{\text { tensor } 1*{i}}{\text { tensor } 2*{i}}$</p>
<h5 id="torch-addcmul"><a href="#torch-addcmul" class="headerlink" title="torch.addcmul()"></a><code>torch.addcmul()</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.addcmul(<span class="built_in">input</span>, tensor1, tensor2, *, value=<span class="number">1</span>, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>计算公式为：out $<em>{i}=$ input $</em>{i}+$ value $\times$ tensor $1*{i} \times$ tensor $2*{i}$</p>
<h3 id="3、线性回归"><a href="#3、线性回归" class="headerlink" title="3、线性回归"></a>3、线性回归</h3><p>线性回归是分析一个变量 ($y$) 与另外一 (多) 个变量 ($x$) 之间的关系的方法。一般可以写成 $y=wx+b$。线性回归的目的就是求解参数$w, b$。</p>
<p>线性回归的求解可以分为 3 步：</p>
<ol>
<li>确定模型：$y=wx+b$</li>
<li>选择损失函数，一般使用均方误差 MSE：$\frac{1}{m} \sum*{i=1}^{m}\left(y*{i}-\hat{y}<em>{i}\right)^{2}$。其中 $ \hat{y}</em>{i} $ 是预测值，$y$ 是真实值。</li>
<li>使用梯度下降法求解梯度 (其中 $lr$ 是学习率)，并更新参数：<ul>
<li>$w = w - lr * w.grad$</li>
<li>$b = b - lr * b.grad$</li>
</ul>
</li>
</ol>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.05</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练数据</span></span><br><span class="line">x = torch.rand(<span class="number">20</span>, <span class="number">1</span>) * <span class="number">10</span>  <span class="comment"># x data (tensor), shape=(20, 1)</span></span><br><span class="line"><span class="comment"># torch.randn(20, 1) 用于添加噪声</span></span><br><span class="line">y = <span class="number">2</span>*x + (<span class="number">5</span> + torch.randn(<span class="number">20</span>, <span class="number">1</span>))  <span class="comment"># y data (tensor), shape=(20, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建线性回归参数</span></span><br><span class="line">w = torch.randn((<span class="number">1</span>), requires_grad=<span class="literal">True</span>) <span class="comment"># 设置梯度求解为 true</span></span><br><span class="line">b = torch.zeros((<span class="number">1</span>), requires_grad=<span class="literal">True</span>) <span class="comment"># 设置梯度求解为 true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代训练 1000 次</span></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播，计算预测值</span></span><br><span class="line">    wx = torch.mul(w, x)</span><br><span class="line">    y_pred = torch.add(wx, b)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 MSE loss</span></span><br><span class="line">    loss = (<span class="number">0.5</span> * (y - y_pred) ** <span class="number">2</span>).mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    b.data.sub_(lr * b.grad)</span><br><span class="line">    w.data.sub_(lr * w.grad)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每次更新参数之后，都要清零张量的梯度</span></span><br><span class="line">    w.grad.zero_()</span><br><span class="line">    b.grad.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图，每隔 20 次重新绘制直线</span></span><br><span class="line">    <span class="keyword">if</span> iteration % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">        plt.plot(x.data.numpy(), y_pred.data.numpy(), <span class="string">&#x27;r-&#x27;</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">2</span>, <span class="number">20</span>, <span class="string">&#x27;Loss=%.4f&#x27;</span> % loss.data.numpy(), fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>:  <span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        plt.xlim(<span class="number">1.5</span>, <span class="number">10</span>)</span><br><span class="line">        plt.ylim(<span class="number">8</span>, <span class="number">28</span>)</span><br><span class="line">        plt.title(<span class="string">&quot;Iteration: &#123;&#125;\nw: &#123;&#125; b: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(iteration, w.data.numpy(), b.data.numpy()))</span><br><span class="line">        plt.pause(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果 MSE 小于 1，则停止训练</span></span><br><span class="line">        <span class="keyword">if</span> loss.data.numpy() &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>训练的直线的可视化如下：</p>
<img src="/2021/04/05/zhang-liang-cao-zuo-yu-xian-xing-hui-gui/linear_regression_training.gif" class="" title="img">

<p>在 80 次的时候，Loss 已经小于 1 了，因此停止了训练。</p>
</div><div class="post-copyright"><blockquote><p>原文作者: 贺同学</p><p>原文链接: <a href="http://clarkhedi.github.io/2021/04/05/zhang-liang-cao-zuo-yu-xian-xing-hui-gui/">http://clarkhedi.github.io/2021/04/05/zhang-liang-cao-zuo-yu-xian-xing-hui-gui/</a></p><p>版权声明: 转载请注明出处(必须保留原文作者署名原文链接)</p></blockquote></div><div class="tags"><a href="/tags/Pytorch/">Pytorch</a></div><div class="post-share"><div class="social-share"><span>分享到:</span></div><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到:</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2021/04/05/ji-suan-tu-yu-dong-tai-tu-ji-zhi/" class="pre">计算图与动态图机制</a><a href="/2021/04/05/tensor-zhang-liang-jie-shao/" class="next">Tensor（张量）介绍</a></div><div id="comments"><div id="container"><script type="text/javascript" src="https://unpkg.com/gitalk/dist/gitalk.min.js?v=2.0.5"></script><script type="text/javascript" src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js?v=2.0.5"></script><script>var gitalk = new Gitalk({
  clientID: 'd6d4fb51a5284e4df5fd',
  clientSecret: '80ba8b4586bfce6df672e814df089b31e49a5549',
  repo: 'Gitalk',
  owner: 'clarkhedi',
  admin: ['clarkhedi'],
  id: md5(window.location.pathname),
  distractionFreeMode: false,
  language: 'zh-CN',
  pagerDirection: 'last'
})
gitalk.render('container')</script></div></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">张量操作与线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%BC%A0%E9%87%8F%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="toc-text">1、张量的操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8B%BC%E6%8E%A5"><span class="toc-text">拼接</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-cat"><span class="toc-text">torch.cat()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-stack"><span class="toc-text">torch.stack()</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%87%E5%88%86"><span class="toc-text">切分</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-chunk"><span class="toc-text">torch.chunk()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-split"><span class="toc-text">torch.split()</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95"><span class="toc-text">索引</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-index-select"><span class="toc-text">torch.index_select()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-mask-select"><span class="toc-text">torch.mask_select()</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%98%E6%8D%A2"><span class="toc-text">变换</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-reshape"><span class="toc-text">torch.reshape()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-transpose"><span class="toc-text">torch.transpose()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-t"><span class="toc-text">torch.t()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-squeeze"><span class="toc-text">torch.squeeze()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-unsqueeze"><span class="toc-text">torch.unsqueeze()</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E5%BC%A0%E9%87%8F%E7%9A%84%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-text">2、张量的数学运算</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-add"><span class="toc-text">torch.add()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-addcdiv"><span class="toc-text">torch.addcdiv()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-addcmul"><span class="toc-text">torch.addcmul()</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">3、线性回归</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/11/18/hexo-start/">Hexo Start</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/16/ge-xing-hua-ding-zhi-ni-de-github-zhu-ye/">2021年，教程 | 个性化定制你的 GitHub 主页</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/06/07/you-mei-de-sublime-text/">优美的Sublime Text</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/05/08/pyqt5-xue-xi-bi-ji-yi/">PyQt5学习笔记（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/17/kaiming-he-chu-shi-hua-de-xue-xi/">Kaiming He初始化的学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/autograd-yu-luo-ji-hui-gui/">autograd与逻辑回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/ji-suan-tu-yu-dong-tai-tu-ji-zhi/">计算图与动态图机制</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/zhang-liang-cao-zuo-yu-xian-xing-hui-gui/">张量操作与线性回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/04/05/tensor-zhang-liang-jie-shao/">Tensor（张量）介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/13/triplet-loss-xue-xi/">triplet-loss学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/GitHub%E5%AD%A6%E4%B9%A0/">GitHub学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo%E5%AD%A6%E4%B9%A0/">Hexo学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Jupyter%E8%AF%AD%E6%B3%95/">Jupyter语法</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux%E5%AD%A6%E4%B9%A0/">Linux学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/OpenCV-Python%E6%89%8B%E8%AE%B0/">OpenCV-Python手记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Python学习笔记</a><span class="category-list-count">8</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/PyQt5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">PyQt5学习笔记</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Pytorch学习笔记</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E7%B1%BB%E6%8A%80%E6%9C%AF/">分类技术</a><span class="category-list-count">14</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E7%B1%BB%E6%8A%80%E6%9C%AF/LBP%E5%AD%A6%E4%B9%A0/">LBP学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E7%B1%BB%E6%8A%80%E6%9C%AF/SVM%E5%85%AB%E8%82%A1/">SVM八股</a><span class="category-list-count">10</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">图像处理学习笔记</a><span class="category-list-count">7</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB/">车牌识别</a><span class="category-list-count">7</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AE%9E%E9%AA%8C%E6%A5%BC%E5%AD%A6%E4%B9%A0/">实验楼学习</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E8%B8%A9%E5%9D%91/">环境搭建踩坑</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/">英语学习</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">论文学习笔记</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/CLBP/" style="font-size: 15px;">CLBP</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">论文学习</a> <a href="/tags/vim/" style="font-size: 15px;">vim</a> <a href="/tags/OpenCV/" style="font-size: 15px;">OpenCV</a> <a href="/tags/PyQt5/" style="font-size: 15px;">PyQt5</a> <a href="/tags/GUI/" style="font-size: 15px;">GUI</a> <a href="/tags/%E8%AE%B0%E5%8D%95%E8%AF%8D/" style="font-size: 15px;">记单词</a> <a href="/tags/Pytorch/" style="font-size: 15px;">Pytorch</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/tags/GitHub/" style="font-size: 15px;">GitHub</a> <a href="/tags/Resume/" style="font-size: 15px;">Resume</a> <a href="/tags/LBP/" style="font-size: 15px;">LBP</a> <a href="/tags/LPR/" style="font-size: 15px;">LPR</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/">2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/">2021</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">36</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">5</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> 友情链接</i></div><ul></ul><a href="https://github.com/chaooo/hexo-theme-BlueLake" title="BlueLake主题" target="_blank">BlueLake主题</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247484443&amp;idx=1&amp;sn=7110e42ef9e95a8c16064dde5b897960&amp;chksm=e870d556df075c4053a90d207c078e5fe965b5f9004de7f34dee09fccf1a37977aa3860b3a96&amp;mpshare=1&amp;scene=23&amp;srcid=0428XfRkvl0HsaHMU71k04dq#rd" title="重磅|完备的AI学习路线,最详细的资源整理！" target="_blank">重磅|完备的AI学习路线,最详细的资源整理！</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzI5NDY1MjQzNA==&amp;mid=2247489096&amp;idx=1&amp;sn=ca2e1d72c9decd021c0365cc9e93a539&amp;chksm=ec5ec935db2940237482c5581b7a45f2bd50b138efa6488ed9b942c182486a4e74131e9af523&amp;mpshare=1&amp;scene=23&amp;srcid=#rd" title="推荐|机器学习入门方法和资料合集" target="_blank">推荐|机器学习入门方法和资料合集</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzU4NTY4Mzg1Mw==&amp;mid=100000497&amp;idx=1&amp;sn=96cc795f1c22f7da2f3260d4ea364ff3&amp;chksm=7d8784134af00d05e41da36475bca417851535ed9710c7bd44ae7365cf40857015d922671c95&amp;mpshare=1&amp;scene=23&amp;srcid=04138MekLgFi3vNAz2E2rTrv#rd" title="良心推荐：机器学习入门资料汇总及学习建议（2018版）" target="_blank">良心推荐：机器学习入门资料汇总及学习建议（2018版）</a><ul></ul><a href="https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&amp;mid=2649032678&amp;idx=1&amp;sn=7ce30241f24dae790f7361173132ee04&amp;chksm=8712b99bb065308d6626ac71d358d386199f5fdcc0d20f48b9ff7f7f84f32892325075413d78&amp;mpshare=1&amp;scene=1&amp;srcid=1213VnRvWgKRhAswA6qj9C6U&amp;sharer_sharetime=1607823235596&amp;sharer_shareid=6f3d2b6e1888cc6674af1cb9d8f856b5&amp;key=94e16379e8fc498f20603adf1c7c1474460f32c861df423924a01860bf66b23e3b54ec715f59513ed57f7be44b91602324165303caa8015279359147bf99dc0ac5c9482bbf323df6415707db1df1dbb44795972834805221b6810c0783bc61213b424afc1f9c747d8eddfe17782e3165ddfff5144eff7867ea82ee34ab2e6e44&amp;ascene=1&amp;uin=MzMyMzAzNzE5OA%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=en&amp;exportkey=A%2Fn4vOWzACQbiXdS3ts1%2BmU%3D&amp;pass_ticket=X0sXcXVfthe9ne%2BkdsNidLJDaUos8pPBg0qSPgQJE8kY3CQmybx%2FB2TVL1w9Glu6&amp;wx_header=0" title="深度学习CV算法工程师从入门到初级面试" target="_blank">深度学习CV算法工程师从入门到初级面试</a></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">贺同学.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a target="_blank" rel="noopener" href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.5"></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="/js/imgview.js?v=2.0.5" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.5" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>